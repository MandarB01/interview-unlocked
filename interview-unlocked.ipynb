{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fd836ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -qU langchain langgraph langgraph-swarm langchain-google-genai langchain_community faiss-cpu tavily-python google-cloud-speech sounddevice scipy pdfminer.six python-dotenv langchain-openai numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d0259e",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Install necessary libraries and import required modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "262ab74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "#%pip install -qU langchain langgraph langgraph-swarm langchain-google-genai langchain_community faiss-cpu tavily-python google-cloud-speech sounddevice scipy pdfminer.six python-dotenv langchain-openai\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import uuid\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import scipy.io.wavfile as wav\n",
    "from typing import List, Dict, Any, Optional, TypedDict\n",
    "\n",
    "# Replace Ollama with Google Generative AI (Gemini)\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "# from langchain_ollama.chat_models import ChatOllama\n",
    "# from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "# Use pydantic.v1 for compatibility as suggested by the warning\n",
    "from pydantic.v1 import BaseModel, Field \n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "# Removed unused langgraph_swarm imports\n",
    "\n",
    "from google.cloud import speech\n",
    "from pdfminer.high_level import extract_text\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (for API keys like Tavily, Google Cloud)\n",
    "load_dotenv()\n",
    "\n",
    "# --- Configuration ---\n",
    "FAISS_RESUME_PATH = \"../faiss/resume_embeddings\"\n",
    "FAISS_JD_PATH = \"../faiss/jd_embeddings\"\n",
    "FAISS_RUBRIC_PATH = \"../faiss/rubric_embeddings\"\n",
    "os.makedirs(os.path.dirname(FAISS_RESUME_PATH), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(FAISS_JD_PATH), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(FAISS_RUBRIC_PATH), exist_ok=True)\n",
    "# Retrieve the API key loaded by load_dotenv()\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Initialize LLM (using Gemini)\n",
    "# Pass the API key explicitly\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-pro\",\n",
    "    temperature=0.1,\n",
    "    convert_system_message_to_human=True,\n",
    "    google_api_key=google_api_key\n",
    ")\n",
    "# llm = ChatOllama(model=\"llama3\", temperature=0.1)\n",
    "\n",
    "# Initialize Embeddings Model (using Gemini)\n",
    "# Pass the API key explicitly\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=google_api_key\n",
    ")\n",
    "# embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# Initialize Checkpointer\n",
    "memory = InMemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fdbac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ed3adb8",
   "metadata": {},
   "source": [
    "Okay, I will update the interview-unlocked.ipynb notebook to implement the agentic workflow described in design.md using `langgraph` and `langgraph_swarm`.\n",
    "\n",
    "Made changes.\n",
    "\n",
    "OK. I have populated the interview-unlocked.ipynb notebook with the Python code to implement the multi-agent interview preparation system based on your design.md file and using LangGraph.\n",
    "\n",
    "The notebook now contains the following sections:\n",
    "\n",
    "1.  **Setup and Imports:** Installs necessary packages (commented out by default) and imports all required libraries and modules. It also sets up configurations like FAISS paths and initializes the LLM and embedding models.\n",
    "2.  **Tool Definitions:** Implements all the tools required by the agents, such as text extraction, embedding generation, vector retrieval, web search, LeetCode retrieval (placeholder), audio recording/transcription, and evaluation helper tools (placeholders).\n",
    "3.  **Agent Definitions:** Defines the `InterviewState` TypedDict to manage the data flow between agents. It includes the prompts for each agent (Preprocessing, Knowledge, Planner, Question, Evaluation) derived from design.md and sets up the agent nodes using LangGraph's `ToolNode` and a helper function.\n",
    "4.  **Graph Definition:** Constructs the `StateGraph`, adds nodes for each agent and its tools, defines the sequential edges connecting them (Preprocessing -> Knowledge -> Planner -> Question -> Record Answer -> Evaluate -> END), and compiles the graph. It also includes code to optionally visualize the graph structure.\n",
    "5.  **Execution and Interaction:** Provides an example of how to run the graph. You'll need to **modify the `resume_file` and `jd_file` variables** to point to your actual resume and job description files. It then runs the graph asynchronously, streams the events (like tool calls and LLM outputs), and finally prints the key outputs extracted from the final state (study plan, suggested LeetCode, generated question, transcribed answer, and evaluation feedback).\n",
    "\n",
    "Please review the notebook, especially the placeholder file paths in the \"Execution and Interaction\" section, and ensure your environment (API keys, Ollama server if used, Google Cloud credentials) is set up correctly before running the cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf2dadb",
   "metadata": {},
   "source": [
    "# Interview Unlocked: Agentic Interview Preparation System\n",
    "\n",
    "This notebook implements a multi-agent system using LangGraph and LangGraph Swarm to help users prepare for job interviews. It follows the design outlined in `design.md`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "362b2961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tool Implementations ---\n",
    "\n",
    "@tool\n",
    "def extract_text_with_ocr(file_path: str) -> str:\n",
    "    \"\"\"Extracts text from a file (PDFs supported). Placeholder for OCR if needed.\"\"\"\n",
    "    try:\n",
    "        if file_path.lower().endswith('.pdf'):\n",
    "            return extract_text(file_path)\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting text: {e}\"\n",
    "\n",
    "@tool\n",
    "def generate_embeddings_and_save(text: str, index_path: str) -> str:\n",
    "    \"\"\"Generates embeddings for the text and saves/updates the FAISS index.\"\"\"\n",
    "    try:\n",
    "        texts = [text] # FAISS expects a list\n",
    "        if os.path.exists(index_path):\n",
    "            vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "            vectorstore.add_texts(texts)\n",
    "        else:\n",
    "            vectorstore = FAISS.from_texts(texts, embeddings)\n",
    "        vectorstore.save_local(index_path)\n",
    "        return f\"Embeddings generated and saved to {index_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error generating/saving embeddings: {e}\"\n",
    "\n",
    "@tool\n",
    "def retrieve_from_vector_db(query: str, index_path: str, k: int = 3) -> List[str]:\n",
    "    \"\"\"Retrieves relevant documents from a FAISS index.\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(index_path):\n",
    "            return [\"Vector index not found.\"]\n",
    "        vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "        results = vectorstore.similarity_search(query, k=k)\n",
    "        return [doc.page_content for doc in results]\n",
    "    except Exception as e:\n",
    "        return [f\"Error retrieving from vector DB: {e}\"]\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the API key from environment variables\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "# Check if the API key was loaded\n",
    "if not tavily_api_key:\n",
    "    raise ValueError(\"TAVILY_API_KEY not found in environment variables. Please ensure it is set in your .env file.\")\n",
    "\n",
    "# Tavily Search Tool (already integrated in LangChain)\n",
    "# Pass the API key during initialization\n",
    "tavily_tool = TavilySearchResults(tavily_api_key=tavily_api_key, max_results=20)\n",
    "\n",
    "@tool\n",
    "def company_leetcode_retriever(company: str, role_keywords: Optional[List[str]] = None) -> List[str]:\n",
    "    \"\"\"Retrieves suggested LeetCode questions for a company (Placeholder).\"\"\"\n",
    "    # In a real implementation, this would parse an Excel file or query a database.\n",
    "    print(f\"Fetching LeetCode questions for {company} (Role: {role_keywords})...\")\n",
    "    # Placeholder data\n",
    "    questions = {\n",
    "        \"google\": [\"Two Sum\", \"LRU Cache\", \"Word Break\"],\n",
    "        \"amazon\": [\"Median of Two Sorted Arrays\", \"Merge K Sorted Lists\", \"Serialize and Deserialize Binary Tree\"],\n",
    "        \"default\": [\"Reverse Linked List\", \"Valid Parentheses\", \"Coin Change\"]\n",
    "    }\n",
    "    return questions.get(company.lower(), questions[\"default\"])\n",
    "\n",
    "@tool\n",
    "def record_and_transcribe_audio(duration: int = 15, fs: int = 16000) -> str:\n",
    "    \"\"\"Records audio from the microphone for a specified duration and transcribes it using Google Cloud Speech-to-Text.\"\"\"\n",
    "    print(f\"Recording audio for {duration} seconds... Speak now!\")\n",
    "    audio_file = f\"/tmp/interview_answer_{uuid.uuid4()}.wav\"\n",
    "    try:\n",
    "        # Record audio\n",
    "        recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='float32')\n",
    "        sd.wait()\n",
    "        # Convert to int16 and save\n",
    "        recording_int16 = np.int16(recording * 32767)\n",
    "        wav.write(audio_file, fs, recording_int16)\n",
    "        print(\"Audio recorded.\")\n",
    "\n",
    "        # Transcribe audio\n",
    "        print(\"Transcribing audio...\")\n",
    "        client = speech.SpeechClient() # Assumes GOOGLE_APPLICATION_CREDENTIALS is set\n",
    "        with open(audio_file, \"rb\") as f:\n",
    "            content = f.read()\n",
    "        audio = speech.RecognitionAudio(content=content)\n",
    "        config = speech.RecognitionConfig(\n",
    "            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "            sample_rate_hertz=fs,\n",
    "            language_code=\"en-US\",\n",
    "            enable_automatic_punctuation=True\n",
    "        )\n",
    "        response = client.recognize(config=config, audio=audio)\n",
    "        os.remove(audio_file) # Clean up temporary file\n",
    "\n",
    "        if not response.results:\n",
    "            print(\"Transcription failed: No speech detected.\")\n",
    "            return \"[No speech detected]\"\n",
    "\n",
    "        transcript = \" \".join([result.alternatives[0].transcript for result in response.results])\n",
    "        print(f\"Transcription complete: {transcript}\")\n",
    "        return transcript.strip()\n",
    "    except Exception as e:\n",
    "        if os.path.exists(audio_file):\n",
    "            os.remove(audio_file)\n",
    "        error_msg = f\"Error during audio recording or transcription: {e}\"\n",
    "        print(error_msg)\n",
    "        return error_msg\n",
    "\n",
    "# --- Tools for Evaluation Agent (Simulated - LLM will act based on prompt) ---\n",
    "# In a full implementation, these might call separate LLMs or specific logic.\n",
    "@tool\n",
    "def retrieve_rubric_snippets(query: str, company_tag: str, top_k: int = 3, index_path: str = FAISS_RUBRIC_PATH) -> str:\n",
    "    \"\"\"Retrieves relevant rubric snippets from the FAISS index.\"\"\"\n",
    "    results = retrieve_from_vector_db(query=f\"{query} {company_tag}\", index_path=index_path, k=top_k)\n",
    "    return \"\\n\".join(results)\n",
    "\n",
    "@tool\n",
    "def generate_ideal_answer(question: str, company_tag: Optional[str] = None) -> str:\n",
    "    \"\"\"Generates an ideal answer to the question (simulated by LLM call).\"\"\"\n",
    "    # This would typically involve another LLM call with specific instructions\n",
    "    # For simplicity here, we'll just return a placeholder or let the main agent handle it.\n",
    "    return f\"[Placeholder: Ideal answer generation for '{question}' considering company '{company_tag}']\"\n",
    "\n",
    "@tool\n",
    "def rewrite_candidate_answer(question: str, candidate_answer: str) -> str:\n",
    "    \"\"\"Rewrites the candidate's answer for improvement (simulated by LLM call).\"\"\"\n",
    "    return f\"[Placeholder: Rewritten version of answer for '{question}']\"\n",
    "\n",
    "@tool\n",
    "def critique_and_advise(question: str, candidate_answer: str, ideal_answer: str, company_tag: Optional[str] = None) -> str:\n",
    "    \"\"\"Provides critique and advice based on the answers (simulated by LLM call).\"\"\"\n",
    "    return f\"[Placeholder: Critique for answer to '{question}' considering company '{company_tag}']\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36b67c2",
   "metadata": {},
   "source": [
    "## 3. Agent Definitions\n",
    "\n",
    "Define the state, prompts, and nodes for each agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd600388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Agent State ---\n",
    "class InterviewState(TypedDict):\n",
    "    messages: List[Any] # Stores the conversation history\n",
    "    user_resume_path: Optional[str]\n",
    "    user_jd_path: Optional[str]\n",
    "    user_resume_text: Optional[str]\n",
    "    user_jd_text: Optional[str]\n",
    "    clean_resume: Optional[str]\n",
    "    clean_jd: Optional[str]\n",
    "    company_name: Optional[str]\n",
    "    knowledge_output: Optional[Dict[str, Any]] # Output from Knowledge Agent\n",
    "    planner_output: Optional[Dict[str, Any]] # Output from Planner Agent (study plan, etc.)\n",
    "    preferred_question_type: Optional[str]\n",
    "    generated_question: Optional[str]\n",
    "    candidate_answer: Optional[str]\n",
    "    evaluation_output: Optional[Dict[str, Any]] # Output from Evaluation Agent\n",
    "    current_agent: str # Tracks which agent's turn it is\n",
    "\n",
    "# --- Agent Prompts (from design.md) ---\n",
    "\n",
    "preprocessing_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are **PreprocessingAgent**, a specialist in structuring resume and\n",
    "job‑description data for downstream analysis.\n",
    "\n",
    "If *resume_file* is not already plain text, call  *extract_text_with_ocr*.\n",
    "\n",
    "Remove headers, footers, duplicate whitespace, and decorative lines.\n",
    "\n",
    "Label the following sections in the resume exactly as:\n",
    "                 **[CONTACT, SUMMARY,PROJECT, EDUCATION, EXPERIENCE, SKILLS]**\n",
    "Label the following sections only in the job description\n",
    "     **[COMPANY NAME, DESCRIPTION, BASIC QUALIFICATIONS,SALARY]**\n",
    "Produce two cleaned texts:\n",
    "      \t• clean_resume\n",
    "      \t• clean_jd\n",
    "Call generate_embeddings **once for each cleaned text**.  \n",
    "   \tThis tool must **persist** the resulting 384‑dimensional vectors to a\n",
    "   \tFAISS index on disk, using the following directories:  \n",
    "   \t     ▸ `./faiss/resume_embeddings` (for **clean_resume**)  \n",
    "   \t     ▸ `./faiss/jd_embeddings`   (for **clean_jd** )\n",
    "\n",
    "Return **only** the JSON object:\n",
    "    \t{\n",
    "               \"clean_jd\": \"<cleaned JD text>\"\n",
    "\t   “comapny_name”:<str>\n",
    "    \t}\n",
    "   \t*Do **not** include the raw embeddings in the JSON.*\n",
    "\n",
    "Finally, invoke `handoff_to_knowledge` to transfer control to **KnowledgeAgent** for downstream reasoning.\n",
    "\"\"\"),\n",
    "    (\"human\", \"Process the resume at {user_resume_path} and the job description at {user_jd_path}.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "knowledge_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are the Knowledge Agent in a multi-agent interview preparation system. Your role is to extract real-world, subjective expectations about how top companies evaluate technical candidates during interviews. You must infer the evaluation rubric and communication expectations from public internet sources using a web search tool.\n",
    "\n",
    "## Tool:\n",
    "### 1. `websearcher_tool` (Powered by Tavily)\n",
    "- Performs web search queries across Reddit, Glassdoor, Blind, Medium, and other sources\n",
    "- Returns concise search result snippets and their URLs\n",
    "- Ideal for extracting subjective insights and real candidate experiences\n",
    "\n",
    "### 2. `generate_embeddings`\n",
    "- Accepts a list of search result snippets (with metadata)\n",
    "- Converts them into vector embeddings using a language model\n",
    "- Persists the vectors to a FAISS index on disk at:  \n",
    "  `./faiss/search_results`\n",
    "- This ensures future agents can perform semantic retrieval over previously seen company content\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs:\n",
    "You will receive the following input parameters:\n",
    "- `target_company` (string): Name of the company being analyzed (e.g., `\"Amazon\"`)\n",
    "- ‘clean_jd’ (string): Job description of the company being analyzed (e.g., `\"Amazon\"`)\n",
    "- `max_snippets` (int): Maximum number of web results to analyze (default: 20)\n",
    "\n",
    "---\n",
    "\n",
    "## Search Strategy:\n",
    "Construct multiple web queries using templates such as:\n",
    "\n",
    "- `{company} coding interview expectations site:reddit.com`\n",
    "- `{company} behavioral interview rubric site:glassdoor.com`\n",
    "- `{company} system design interview site:blind.com`\n",
    "- `{company} coding round tips site:medium.com`\n",
    "- `{company} leetcode discussion interview prep`\n",
    "\n",
    "You may dynamically vary these queries to maximize rubric signal strength.\n",
    "\n",
    "---\n",
    "\n",
    "## Goal:\n",
    "1. Search for public reflections and advice related to the target company’s interview process.\n",
    "2. Analyze these results to extract subjective behavioral expectations — not problem solutions.\n",
    "3. Identify 2–4 rubric-like themes (e.g., \"ownership\", \"handling ambiguity\", \"tradeoff thinking\").\n",
    "4. Generate 2–4 practical **communication tips** that reflect how a candidate should approach technical problems at that company.\n",
    "5. Always cite a source URL (Reddit, Glassdoor, etc.) for each rubric inference.\n",
    "\n",
    "---\n",
    "\n",
    "## Output Format:\n",
    "Return a JSON object like the following:\n",
    "```json\n",
    "{\n",
    "  \"company\": \"Amazon\",\n",
    "  \"inferred_rubric\": [\n",
    "    {\n",
    "      \"theme\": \"Ownership\",\n",
    "      \"evidence\": \"Reddit users frequently mention that Amazon interviewers expect you to explain fallback strategies and consider constraints proactively.\",\n",
    "      \"discussion_reference\": \"https://www.reddit.com/r/csMajors/comments/abc123\"\n",
    "    },\n",
    "    {\n",
    "      \"theme\": \"Structured Thinking\",\n",
    "      \"evidence\": \"Glassdoor reviews highlight the importance of walking through your approach step-by-step even under pressure.\",\n",
    "      \"discussion_reference\": \"https://www.glassdoor.com/Interview/Amazon-Interview-Questions.htm\"\n",
    "    }\n",
    "  ],\n",
    "  \"communication_tips\": [\n",
    "    \"Explain your thought process clearly before writing code.\",\n",
    "    \"Use tradeoffs to justify your chosen solution.\",\n",
    "    \"Don’t panic if you don’t know the optimal solution — narrate your reasoning and explore alternatives.\",\n",
    "    \"Pause briefly to organize your explanation if asked to clarify.\"\n",
    "  ]\n",
    "}\n",
    "\n",
    "\"\"\"),\n",
    "    (\"human\", \"Generate interview insights for {company_name}. Resume context (optional): {user_resume_text}. JD context (optional): {clean_jd}\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are the Planner Agent, the central orchestrator in a multi-agent interview preparation system. Your primary goal is to synthesize information about the user, the target job, the company's expectations, and user preferences to create a personalized study plan and guide the interview practice session by tasking the Question Agent.\n",
    "Inputs You Will Receive:\n",
    "Contextual Data (from Vector Database / FAISS):\n",
    "Parsed Resume: Key skills, experiences, and project summaries extracted from the user's resume.\n",
    "Parsed Job Description (JD): Required skills, responsibilities, and keywords for the target role.\n",
    "Output from Knowledge Agent:\n",
    "A JSON object containing:\n",
    "inferred_rubric: Themes (e.g., Ownership, Communication) the company implicitly values, with supporting evidence and sources.\n",
    "communication_tips: Actionable advice on how to communicate effectively during the interview, based on community insights.\n",
    "company: Name of the target company.\n",
    "User Preference:\n",
    "preferred_question_type: A string indicating the type of question the user wants to practice next (e.g., \"technical\", \"behavioral\", \"system design\", \"debugging/problem-solving\").\n",
    "Your Core Responsibilities:\n",
    "Synthesize Information: Analyze the Resume, JD, inferred rubric, communication tips, and user preference to understand the alignment between the user's background and the role/company requirements.\n",
    "Generate Study Plan: Create a concise, actionable study plan for the user's asynchronous preparation. This plan should highlight key areas to focus on based on the JD, resume gaps, and inferred company expectations.\n",
    "Suggest LeetCode Questions: Utilize the Company LeetCode Retriever tool to recommend a relevant set of LeetCode problems tailored to the company and the technical requirements of the role.\n",
    "Present Company Insights: Format and clearly present the inferred_rubric and communication_tips received from the Knowledge Agent to the user.\n",
    "Task the Question Agent: Based on the preferred_question_type provided by the user, formulate and send a precise instruction to the Question Agent, specifying the type of question to be generated. Ensure the Question Agent has the necessary context (implicitly, via access to the shared Vector DB).\n",
    "Update Knowledge Base: Use the Vector Database API tool to embed the inferred_rubric and communication_tips into the vector database, making this valuable company-specific context available for future retrieval, especially by the Question Agent.\n",
    "\"\"\"),\n",
    "    (\"human\", \"Plan the interview prep for {company_name} based on the available context and user preference for a '{preferred_question_type}' question. Knowledge Agent output: {knowledge_output}\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "question_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are a smart interview question generation AI agent.\n",
    "You receive an instruction from the Planner Agent specifying the type of question to generate (e.g., technical, behavioral, system design, scenario-based, etc.).\n",
    "You are also required to fetch contextual knowledge via an API call to a vector database. This database contains semantically indexed data including:\n",
    "The job description for the target role.\n",
    "\n",
    "\n",
    "The candidate’s resume and relevant past experience.\n",
    "\n",
    "\n",
    "LeetCode discussion threads and common interview patterns for the target company.\n",
    "\n",
    "\n",
    "Use this data to gain a comprehensive understanding of:\n",
    "The skills, technologies, and responsibilities required by the job.\n",
    "\n",
    "\n",
    "The candidate’s strengths, previous work, and technical familiarity.\n",
    "\n",
    "\n",
    "The company’s preferred question formats and topical focus areas.\n",
    "\n",
    "\n",
    "Your goal: Generate a single, relevant, open-ended interview question that aligns with:\n",
    "The company’s technical and cultural expectations.\n",
    "\n",
    "\n",
    "The candidate’s experience and strengths (to make it targeted and fair).\n",
    "\n",
    "\n",
    "The requested question type from the Planner Agent.\n",
    "\n",
    "\n",
    "Keep the question clear, professional, and appropriate for a real interview. Avoid vague or generic phrasing. Reflect real-world relevance whenever possible.\n",
    "\n",
    "\n",
    "\"\"\"),\n",
    "    (\"human\", \"Generate a '{preferred_question_type}' question for {company_name} based on the available context.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "evaluation_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are *EvaluationFeedbackAgent*, a senior interview coach.\n",
    "Given:\n",
    "  • question\n",
    "  • candidate_answer\n",
    "  • company_tag  (may be empty)\n",
    "  • rubric_index_path          (path to FAISS index)\n",
    "Do the following:\n",
    "1 Call\n",
    "      retrieve_rubric_snippets(question, company_tag, 3, rubric_index_path)\n",
    "   → rubric_snippets\n",
    "   (These contain the company’s evaluation criteria, sample phrases,\n",
    "    leadership principles, etc.)\n",
    "\n",
    "2 Call `generate_ideal_answer(question, company_tag)`  → ideal_answer.\n",
    "3 Call\n",
    "       rewrite_candidate_answer(question, candidate_answer)\n",
    "       improved_answer.\n",
    "4 Call\n",
    "       critique_and_advise(question,\n",
    "                           candidate_answer,\n",
    "                           ideal_answer,\n",
    "                           company_tag)\n",
    "    → detailed_feedback.\n",
    "\n",
    "   The critique must:\n",
    "     • Highlight strengths in the candidate’s answer.\n",
    "     • List missed elements (e.g., time complexity, key verbs such as\n",
    "       \"orchestrated / implemented\").\n",
    "     • Suggest adding a concise STAR story if none is present, giving a\n",
    "       short example story relevant to the question (e.g., BFS usage).\n",
    "     • Use bullet points for clarity and bold key technical terms.\n",
    "\n",
    "4 Return **only** this JSON:\n",
    "    {\n",
    "      \"ideal_answer\": \"<ideal>\",\n",
    "      \"improved_answer\": \"<rewrite>\",\n",
    "      \"detailed_feedback\": \"<critique>\"\n",
    "    }\n",
    "\n",
    "\"\"\"),\n",
    "    (\"human\", \"Evaluate the answer '{candidate_answer}' for the question '{generated_question}' for company '{company_name}'.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "# --- Agent Nodes ---\n",
    "# Helper to create agent nodes\n",
    "def create_agent_node(prompt: ChatPromptTemplate, tools: List[Any]):\n",
    "    agent = prompt | llm.bind_tools(tools)\n",
    "    tool_executor = ToolNode(tools)\n",
    "    async def agent_node(state: InterviewState, config: dict):\n",
    "        result = await agent.ainvoke(state, config)\n",
    "        # If tool calls are requested\n",
    "        if isinstance(result, ToolMessage) or (isinstance(result, AIMessage) and result.tool_calls):\n",
    "             # We delegate to the ToolNode\n",
    "            return {\"messages\": [result]}\n",
    "        else:\n",
    "            # If no tool calls, return the result directly\n",
    "            # Attempt to parse JSON output if expected\n",
    "            try:\n",
    "                if isinstance(result.content, str) and result.content.strip().startswith('{'):\n",
    "                    parsed_output = json.loads(result.content)\n",
    "                    # Update state based on agent\n",
    "                    agent_name = state.get('current_agent', 'unknown')\n",
    "                    if agent_name == 'preprocess':\n",
    "                        state['clean_jd'] = parsed_output.get('clean_jd')\n",
    "                        state['company_name'] = parsed_output.get('company_name')\n",
    "                    elif agent_name == 'knowledge':\n",
    "                        state['knowledge_output'] = parsed_output\n",
    "                    elif agent_name == 'planner':\n",
    "                        state['planner_output'] = parsed_output\n",
    "                    elif agent_name == 'question':\n",
    "                        state['generated_question'] = parsed_output.get('question')\n",
    "                    elif agent_name == 'evaluate':\n",
    "                        state['evaluation_output'] = parsed_output\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: Could not parse JSON output from {state.get('current_agent', 'agent')}\")\n",
    "                # Fallback or handle error as needed\n",
    "            return {\"messages\": [result]}\n",
    "    return agent_node, tool_executor\n",
    "\n",
    "# Create nodes\n",
    "preprocess_tools = [extract_text_with_ocr, generate_embeddings_and_save]\n",
    "preprocess_agent_node, preprocess_tool_node = create_agent_node(preprocessing_prompt, preprocess_tools)\n",
    "\n",
    "knowledge_tools = [tavily_tool]\n",
    "knowledge_agent_node, knowledge_tool_node = create_agent_node(knowledge_prompt, knowledge_tools)\n",
    "\n",
    "planner_tools = [company_leetcode_retriever, generate_embeddings_and_save]\n",
    "planner_agent_node, planner_tool_node = create_agent_node(planner_prompt, planner_tools)\n",
    "\n",
    "question_tools = [retrieve_from_vector_db]\n",
    "question_agent_node, question_tool_node = create_agent_node(question_prompt, question_tools)\n",
    "\n",
    "evaluation_tools = [retrieve_rubric_snippets, generate_ideal_answer, rewrite_candidate_answer, critique_and_advise]\n",
    "evaluation_agent_node, evaluation_tool_node = create_agent_node(evaluation_prompt, evaluation_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a761bab",
   "metadata": {},
   "source": [
    "## 4. Graph Definition\n",
    "\n",
    "Define the workflow connecting the agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cba1e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Graph Construction ---\n",
    "workflow = StateGraph(InterviewState)\n",
    "\n",
    "# Add nodes for each agent and their tools\n",
    "workflow.add_node(\"preprocess\", preprocess_agent_node)\n",
    "workflow.add_node(\"preprocess_tools\", preprocess_tool_node)\n",
    "workflow.add_node(\"knowledge\", knowledge_agent_node)\n",
    "workflow.add_node(\"knowledge_tools\", knowledge_tool_node)\n",
    "workflow.add_node(\"planner\", planner_agent_node)\n",
    "workflow.add_node(\"planner_tools\", planner_tool_node)\n",
    "workflow.add_node(\"question\", question_agent_node)\n",
    "workflow.add_node(\"question_tools\", question_tool_node)\n",
    "workflow.add_node(\"record_answer\", record_and_transcribe_audio) # Direct tool call node\n",
    "workflow.add_node(\"evaluate\", evaluation_agent_node)\n",
    "workflow.add_node(\"evaluate_tools\", evaluation_tool_node)\n",
    "\n",
    "# Define edges\n",
    "workflow.set_entry_point(\"preprocess\")\n",
    "\n",
    "# Preprocessing Agent Logic\n",
    "workflow.add_edge(\"preprocess\", \"preprocess_tools\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"preprocess_tools\",\n",
    "    tools_condition,\n",
    "    {\"continue\": \"knowledge\", END: END} # If tool call needed, loop back via tools_condition, else go to knowledge\n",
    ")\n",
    "\n",
    "# Knowledge Agent Logic\n",
    "workflow.add_edge(\"knowledge\", \"knowledge_tools\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"knowledge_tools\",\n",
    "    tools_condition,\n",
    "    {\"continue\": \"planner\", END: END}\n",
    ")\n",
    "\n",
    "# Planner Agent Logic\n",
    "workflow.add_edge(\"planner\", \"planner_tools\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"planner_tools\",\n",
    "    tools_condition,\n",
    "    {\"continue\": \"question\", END: END}\n",
    ")\n",
    "\n",
    "# Question Agent Logic\n",
    "workflow.add_edge(\"question\", \"question_tools\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"question_tools\",\n",
    "    tools_condition,\n",
    "    {\"continue\": \"record_answer\", END: END} # After question is generated, record answer\n",
    ")\n",
    "\n",
    "# Record Answer Node\n",
    "workflow.add_edge(\"record_answer\", \"evaluate\") # After recording, go to evaluation\n",
    "\n",
    "# Evaluation Agent Logic\n",
    "workflow.add_edge(\"evaluate\", \"evaluate_tools\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"evaluate_tools\",\n",
    "    tools_condition,\n",
    "    {\"continue\": END, END: END} # End after evaluation\n",
    ")\n",
    "\n",
    "# Compile the graph\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "\n",
    "print(\"Graph compiled successfully!\")\n",
    "# Optional: Visualize the graph\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph: {e}. Make sure graphviz and mermaid are installed/configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9011b1",
   "metadata": {},
   "source": [
    "## 5. Execution and Interaction\n",
    "\n",
    "Run the graph with user inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import pprint\n",
    "\n",
    "# --- Execution ---\n",
    "\n",
    "# IMPORTANT: Set the path to your Google Cloud credentials file\n",
    "# This is needed for the record_and_transcribe_audio tool\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"key.json\" # Replace with the actual path to your key.json\n",
    "\n",
    "# --- User Inputs ---\n",
    "# !!! IMPORTANT: Replace these with the actual paths to your files !!!\n",
    "resume_file = \"placeholder_resume.pdf\" # e.g., \"/path/to/your/resume.pdf\"\n",
    "jd_file = \"amazon-jd.txt\"  ., \"/path/to/your/job_description.txt\"\n",
    "user_preferred_question_type = \"technical\" # Options: \"technical\", \"behavioral\", \"system design\", \"debugging/problem-solving\"\n",
    "\n",
    "# Create dummy files if they don't exist for the example run\n",
    "if not os.path.exists(resume_file):\n",
    "    with open(resume_file, \"w\") as f:\n",
    "        f.write(\"Sample Resume Content: Python Developer with 5 years experience in web development and data analysis.\")\n",
    "if not os.path.exists(jd_file):\n",
    "     with open(jd_file, \"w\") as f:\n",
    "        f.write(\"Sample Job Description: Looking for a Senior Software Engineer at Google. Requires strong Python skills, experience with distributed systems, and cloud platforms.\")\n",
    "\n",
    "# Define the initial state to start the graph\n",
    "initial_state = {\n",
    "    \"messages\": [],\n",
    "    \"user_resume_path\": resume_file,\n",
    "    \"user_jd_path\": jd_file,\n",
    "    \"preferred_question_type\": user_preferred_question_type,\n",
    "    \"current_agent\": \"preprocess\" # Start with the preprocessing agent\n",
    "}\n",
    "\n",
    "# Configuration for the graph run (e.g., unique thread ID)\n",
    "config = {\"configurable\": {\"thread_id\": \"interview-prep-thread-1\"}}\n",
    "\n",
    "async def run_graph():\n",
    "    final_state = None\n",
    "    print(\"--- Starting Interview Prep Workflow ---\")\n",
    "    print(f\"Resume: {resume_file}\")\n",
    "    print(f\"Job Description: {jd_file}\")\n",
    "    print(f\"Preferred Question Type: {user_preferred_question_type}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    async for event in graph.astream_events(initial_state, config, version=\"v1\"):\n",
    "        kind = event[\"event\"]\n",
    "        tags = event.get(\"tags\", [])\n",
    "        if kind == \"on_chat_model_stream\":\n",
    "            content = event[\"data\"][\"chunk\"].content\n",
    "            if content:\n",
    "                # Print LLM tokens as they arrive\n",
    "                print(content, end=\"|\")\n",
    "        elif kind == \"on_tool_start\":\n",
    "            print(f\"\\n--- Calling Tool: {event['name']} ---\")\n",
    "            print(f\"   Args: {event['data'].get('input')}\")\n",
    "        elif kind == \"on_tool_end\":\n",
    "            print(f\"--- Tool Result: {event['name']} ---\")\n",
    "            print(f\"   Output: {event['data'].get('output')}\")\n",
    "            print(\"-\" * 30)\n",
    "        elif kind == \"on_chain_end\":\n",
    "             # Check if it's the end of a specific agent node run\n",
    "            if event[\"name\"] in [\"preprocess\", \"knowledge\", \"planner\", \"question\", \"evaluate\"]:\n",
    "                 print(f\"\\n--- Finished Agent: {event['name']} ---\")\n",
    "                 # pprint.pprint(event['data'].get('output'), indent=2) # Print agent output if needed\n",
    "                 print(\"-\" * 30)\n",
    "\n",
    "\n",
    "        # Track the final state\n",
    "        if kind == \"on_graph_end\":\n",
    "            final_state = event['data']['output']\n",
    "\n",
    "\n",
    "    print(\"\\n--- Workflow Complete ---\")\n",
    "\n",
    "    if final_state:\n",
    "        print(\"\\n--- Final Results ---\")\n",
    "        # Extract and print key information from the final state\n",
    "        planner_output = final_state.get('planner_output', {})\n",
    "        evaluation_output = final_state.get('evaluation_output', {})\n",
    "\n",
    "        print(\"\\n**Study Plan:**\")\n",
    "        print(planner_output.get('study_plan', 'Not generated.'))\n",
    "\n",
    "        print(\"\\n**Suggested LeetCode:**\")\n",
    "        pprint.pprint(planner_output.get('suggested_leetcode', 'Not generated.'))\n",
    "\n",
    "        print(\"\\n**Company Insights:**\")\n",
    "        pprint.pprint(planner_output.get('company_insights_display', 'Not generated.'))\n",
    "\n",
    "        print(f\"\\n**Generated Question ({final_state.get('preferred_question_type', 'N/A')}):**\")\n",
    "        print(final_state.get('generated_question', 'Not generated.'))\n",
    "\n",
    "        print(\"\\n**Your Transcribed Answer:**\")\n",
    "        # The actual transcribed answer isn't directly stored in the state by the tool node,\n",
    "        # but it was passed to the evaluation agent. We print the placeholder for clarity.\n",
    "        # In a real UI, you'd capture the output of the 'record_answer' node.\n",
    "        print(final_state.get('candidate_answer', '[Answer was recorded and passed to evaluation]'))\n",
    "\n",
    "\n",
    "        print(\"\\n**Evaluation Feedback:**\")\n",
    "        print(\"\\n*Ideal Answer (Placeholder):*\")\n",
    "        print(evaluation_output.get('ideal_answer', 'Not generated.'))\n",
    "        print(\"\\n*Improved Answer (Placeholder):*\")\n",
    "        print(evaluation_output.get('improved_answer', 'Not generated.'))\n",
    "        print(\"\\n*Detailed Feedback (Placeholder):*\")\n",
    "        print(evaluation_output.get('detailed_feedback', 'Not generated.'))\n",
    "    else:\n",
    "        print(\"Workflow did not complete successfully or final state not captured.\")\n",
    "\n",
    "# Run the asynchronous function\n",
    "try:\n",
    "    asyncio.run(run_graph())\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during graph execution: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
