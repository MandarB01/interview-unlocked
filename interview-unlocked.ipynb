{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6fd836ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745126316.690518 13948258 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -qU langchain langgraph langgraph-swarm langchain-google-genai google-generativeai langchain_community faiss-cpu tavily-python google-cloud-speech sounddevice scipy pdfminer.six python-dotenv langchain-openai numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d0259e",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Install necessary libraries and import required modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "262ab74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install -qU langchain langgraph langgraph-swarm langchain-google-genai langchain_community faiss-cpu tavily-python google-cloud-speech sounddevice scipy pdfminer.six python-dotenv langchain-openai\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import uuid\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import scipy.io.wavfile as wav\n",
    "from typing import List, Dict, Any, Optional, TypedDict\n",
    "\n",
    "# Replace Ollama with Google Generative AI (Gemini)\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "# from langchain_ollama.chat_models import ChatOllama\n",
    "# from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "# Use pydantic.v1 for compatibility as suggested by the warning\n",
    "from pydantic.v1 import BaseModel, Field \n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "# Removed unused langgraph_swarm imports\n",
    "\n",
    "from google.cloud import speech\n",
    "from pdfminer.high_level import extract_text\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (for API keys like Tavily, Google Cloud)\n",
    "load_dotenv()\n",
    "\n",
    "# --- Configuration ---\n",
    "FAISS_RESUME_PATH = \"../faiss/resume_embeddings\"\n",
    "FAISS_JD_PATH = \"../faiss/jd_embeddings\"\n",
    "FAISS_RUBRIC_PATH = \"../faiss/rubric_embeddings\"\n",
    "os.makedirs(os.path.dirname(FAISS_RESUME_PATH), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(FAISS_JD_PATH), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(FAISS_RUBRIC_PATH), exist_ok=True)\n",
    "# Retrieve the API key loaded by load_dotenv()\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Initialize LLM (using Gemini)\n",
    "# Pass the API key explicitly\n",
    "# import google.generativeai as genai\n",
    "# genai.configure(api_key=google_api_key)\n",
    "# models = genai.list_models()\n",
    "\n",
    "# for m in models:\n",
    "#     print(m.name, m.supported_generation_methods)\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",  # Must be valid in your project\n",
    "    temperature=0.1,\n",
    "    convert_system_message_to_human=True,\n",
    "    google_api_key=google_api_key  # from .env\n",
    ")\n",
    "# llm = ChatOllama(model=\"llama3\", temperature=0.1)\n",
    "\n",
    "# Initialize Embeddings Model (using Gemini)\n",
    "# Pass the API key explicitly\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=google_api_key\n",
    ")\n",
    "# embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# Initialize Checkpointer\n",
    "memory = InMemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fdbac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ed3adb8",
   "metadata": {},
   "source": [
    "Okay, I will update the interview-unlocked.ipynb notebook to implement the agentic workflow described in design.md using `langgraph` and `langgraph_swarm`.\n",
    "\n",
    "Made changes.\n",
    "\n",
    "OK. I have populated the interview-unlocked.ipynb notebook with the Python code to implement the multi-agent interview preparation system based on your design.md file and using LangGraph.\n",
    "\n",
    "The notebook now contains the following sections:\n",
    "\n",
    "1.  **Setup and Imports:** Installs necessary packages (commented out by default) and imports all required libraries and modules. It also sets up configurations like FAISS paths and initializes the LLM and embedding models.\n",
    "2.  **Tool Definitions:** Implements all the tools required by the agents, such as text extraction, embedding generation, vector retrieval, web search, LeetCode retrieval (placeholder), audio recording/transcription, and evaluation helper tools (placeholders).\n",
    "3.  **Agent Definitions:** Defines the `InterviewState` TypedDict to manage the data flow between agents. It includes the prompts for each agent (Preprocessing, Knowledge, Planner, Question, Evaluation) derived from design.md and sets up the agent nodes using LangGraph's `ToolNode` and a helper function.\n",
    "4.  **Graph Definition:** Constructs the `StateGraph`, adds nodes for each agent and its tools, defines the sequential edges connecting them (Preprocessing -> Knowledge -> Planner -> Question -> Record Answer -> Evaluate -> END), and compiles the graph. It also includes code to optionally visualize the graph structure.\n",
    "5.  **Execution and Interaction:** Provides an example of how to run the graph. You'll need to **modify the `resume_file` and `jd_file` variables** to point to your actual resume and job description files. It then runs the graph asynchronously, streams the events (like tool calls and LLM outputs), and finally prints the key outputs extracted from the final state (study plan, suggested LeetCode, generated question, transcribed answer, and evaluation feedback).\n",
    "\n",
    "Please review the notebook, especially the placeholder file paths in the \"Execution and Interaction\" section, and ensure your environment (API keys, Ollama server if used, Google Cloud credentials) is set up correctly before running the cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf2dadb",
   "metadata": {},
   "source": [
    "# Interview Unlocked: Agentic Interview Preparation System\n",
    "\n",
    "This notebook implements a multi-agent system using LangGraph and LangGraph Swarm to help users prepare for job interviews. It follows the design outlined in `design.md`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "362b2961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tool Implementations ---\n",
    "\n",
    "@tool\n",
    "def extract_text_with_ocr(file_path: str) -> str:\n",
    "    \"\"\"Extracts text from a file (PDFs supported). Placeholder for OCR if needed.\"\"\"\n",
    "    try:\n",
    "        if file_path.lower().endswith('.pdf'):\n",
    "            return extract_text(file_path)\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting text: {e}\"\n",
    "\n",
    "@tool\n",
    "def generate_embeddings_and_save(text: str, index_path: str) -> str:\n",
    "    \"\"\"Generates embeddings for the text and saves/updates the FAISS index.\"\"\"\n",
    "    try:\n",
    "        texts = [text] # FAISS expects a list\n",
    "        if os.path.exists(index_path):\n",
    "            vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "            vectorstore.add_texts(texts)\n",
    "        else:\n",
    "            vectorstore = FAISS.from_texts(texts, embeddings)\n",
    "        vectorstore.save_local(index_path)\n",
    "        return f\"Embeddings generated and saved to {index_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error generating/saving embeddings: {e}\"\n",
    "\n",
    "@tool\n",
    "def retrieve_from_vector_db(query: str, index_path: str, k: int = 3) -> List[str]:\n",
    "    \"\"\"Retrieves relevant documents from a FAISS index.\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(index_path):\n",
    "            return [\"Vector index not found.\"]\n",
    "        vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "        results = vectorstore.similarity_search(query, k=k)\n",
    "        return [doc.page_content for doc in results]\n",
    "    except Exception as e:\n",
    "        return [f\"Error retrieving from vector DB: {e}\"]\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the API key from environment variables\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "# Check if the API key was loaded\n",
    "if not tavily_api_key:\n",
    "    raise ValueError(\"TAVILY_API_KEY not found in environment variables. Please ensure it is set in your .env file.\")\n",
    "\n",
    "# Tavily Search Tool (already integrated in LangChain)\n",
    "# Pass the API key during initialization\n",
    "tavily_tool = TavilySearchResults(tavily_api_key=tavily_api_key, max_results=20)\n",
    "\n",
    "@tool\n",
    "def company_leetcode_retriever(company: str, role_keywords: Optional[List[str]] = None) -> List[str]:\n",
    "    \"\"\"Retrieves suggested LeetCode questions for a company (Placeholder).\"\"\"\n",
    "    # In a real implementation, this would parse an Excel file or query a database.\n",
    "    print(f\"Fetching LeetCode questions for {company} (Role: {role_keywords})...\")\n",
    "    # Placeholder data\n",
    "    questions = {\n",
    "        \"google\": [\"Two Sum\", \"LRU Cache\", \"Word Break\"],\n",
    "        \"amazon\": [\"Median of Two Sorted Arrays\", \"Merge K Sorted Lists\", \"Serialize and Deserialize Binary Tree\"],\n",
    "        \"default\": [\"Reverse Linked List\", \"Valid Parentheses\", \"Coin Change\"]\n",
    "    }\n",
    "    return questions.get(company.lower(), questions[\"default\"])\n",
    "\n",
    "@tool\n",
    "def record_and_transcribe_audio(duration: int = 15, fs: int = 16000) -> str:\n",
    "    \"\"\"Records audio from the microphone for a specified duration and transcribes it using Google Cloud Speech-to-Text.\"\"\"\n",
    "    print(f\"Recording audio for {duration} seconds... Speak now!\")\n",
    "    audio_file = f\"/tmp/interview_answer_{uuid.uuid4()}.wav\"\n",
    "    try:\n",
    "        # Record audio\n",
    "        recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='float32')\n",
    "        sd.wait()\n",
    "        # Convert to int16 and save\n",
    "        recording_int16 = np.int16(recording * 32767)\n",
    "        wav.write(audio_file, fs, recording_int16)\n",
    "        print(\"Audio recorded.\")\n",
    "\n",
    "        # Transcribe audio\n",
    "        print(\"Transcribing audio...\")\n",
    "        client = speech.SpeechClient() # Assumes GOOGLE_APPLICATION_CREDENTIALS is set\n",
    "        with open(audio_file, \"rb\") as f:\n",
    "            content = f.read()\n",
    "        audio = speech.RecognitionAudio(content=content)\n",
    "        config = speech.RecognitionConfig(\n",
    "            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "            sample_rate_hertz=fs,\n",
    "            language_code=\"en-US\",\n",
    "            enable_automatic_punctuation=True\n",
    "        )\n",
    "        response = client.recognize(config=config, audio=audio)\n",
    "        os.remove(audio_file) # Clean up temporary file\n",
    "\n",
    "        if not response.results:\n",
    "            print(\"Transcription failed: No speech detected.\")\n",
    "            return \"[No speech detected]\"\n",
    "\n",
    "        transcript = \" \".join([result.alternatives[0].transcript for result in response.results])\n",
    "        print(f\"Transcription complete: {transcript}\")\n",
    "        return transcript.strip()\n",
    "    except Exception as e:\n",
    "        if os.path.exists(audio_file):\n",
    "            os.remove(audio_file)\n",
    "        error_msg = f\"Error during audio recording or transcription: {e}\"\n",
    "        print(error_msg)\n",
    "        return error_msg\n",
    "\n",
    "# --- Tools for Evaluation Agent (Simulated - LLM will act based on prompt) ---\n",
    "# In a full implementation, these might call separate LLMs or specific logic.\n",
    "@tool\n",
    "def retrieve_rubric_snippets(query: str, company_tag: str, top_k: int = 3, index_path: str = FAISS_RUBRIC_PATH) -> str:\n",
    "    \"\"\"Retrieves relevant rubric snippets from the FAISS index.\"\"\"\n",
    "    results = retrieve_from_vector_db(query=f\"{query} {company_tag}\", index_path=index_path, k=top_k)\n",
    "    return \"\\n\".join(results)\n",
    "\n",
    "@tool\n",
    "def generate_ideal_answer(question: str, company_tag: Optional[str] = None) -> str:\n",
    "    \"\"\"Generates an ideal answer to the question (simulated by LLM call).\"\"\"\n",
    "    # This would typically involve another LLM call with specific instructions\n",
    "    # For simplicity here, we'll just return a placeholder or let the main agent handle it.\n",
    "    return f\"[Placeholder: Ideal answer generation for '{question}' considering company '{company_tag}']\"\n",
    "\n",
    "@tool\n",
    "def rewrite_candidate_answer(question: str, candidate_answer: str) -> str:\n",
    "    \"\"\"Rewrites the candidate's answer for improvement (simulated by LLM call).\"\"\"\n",
    "    return f\"[Placeholder: Rewritten version of answer for '{question}']\"\n",
    "\n",
    "@tool\n",
    "def critique_and_advise(question: str, candidate_answer: str, ideal_answer: str, company_tag: Optional[str] = None) -> str:\n",
    "    \"\"\"Provides critique and advice based on the answers (simulated by LLM call).\"\"\"\n",
    "    return f\"[Placeholder: Critique for answer to '{question}' considering company '{company_tag}']\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36b67c2",
   "metadata": {},
   "source": [
    "## 3. Agent Definitions\n",
    "\n",
    "Define the state, prompts, and nodes for each agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cd600388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Agent State ---\n",
    "class InterviewState(TypedDict):\n",
    "    messages: List[Any] # Stores the conversation history\n",
    "    user_resume_path: Optional[str]\n",
    "    user_jd_path: Optional[str]\n",
    "    user_resume_text: Optional[str]\n",
    "    user_jd_text: Optional[str]\n",
    "    clean_resume: Optional[str]\n",
    "    clean_jd: Optional[str]\n",
    "    company_name: Optional[str]\n",
    "    knowledge_output: Optional[Dict[str, Any]] # Output from Knowledge Agent\n",
    "    planner_output: Optional[Dict[str, Any]] # Output from Planner Agent (study plan, etc.)\n",
    "    preferred_question_type: Optional[str]\n",
    "    generated_question: Optional[str]\n",
    "    candidate_answer: Optional[str]\n",
    "    evaluation_output: Optional[Dict[str, Any]] # Output from Evaluation Agent\n",
    "    current_agent: str # Tracks which agent's turn it is\n",
    "\n",
    "# --- Agent Prompts (from design.md) ---\n",
    "\n",
    "preprocessing_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are PreprocessingAgent, a specialist in structuring resume and job-description data for downstream analysis.\n",
    "You will be given paths to the user's resume and the job description.\n",
    "\n",
    "1. Call `extract_text_with_ocr` for both the resume file (`user_resume_path`) and the job description file (`user_jd_path`).\n",
    "2. Clean the extracted text: Remove headers, footers, duplicate whitespace, and decorative lines. Store these as `clean_resume` and `clean_jd`.\n",
    "   *(Self-correction: Labeling sections like [CONTACT...] is complex and better handled by downstream agents if needed. Focus on cleaning and embedding.)*\n",
    "3. Extract the 'COMPANY NAME' from the cleaned job description (`clean_jd`).\n",
    "4. Call `generate_embeddings_and_save` **once for `clean_resume`**, saving to ../faiss/resume_embeddings.\n",
    "5. Call `generate_embeddings_and_save` **once for `clean_jd`**, saving to ../faiss/jd_embeddings .\n",
    "6. Return **only** the JSON object containing the cleaned JD and company name:\n",
    "   ```json\n",
    "   {{\n",
    "     \"clean_jd\": \"<cleaned job description text>\",\n",
    "     \"company_name\": \"<extracted company name>\"\n",
    "   }}\n",
    "   ```\n",
    "   *Do not include the raw embeddings or cleaned resume in the final JSON output of this step.*\n",
    "7. This output signals completion. The next agent (KnowledgeAgent) will be invoked by the graph.\n",
    "\"\"\"),\n",
    "    (\"human\", \"Process the resume at {user_resume_path} and the job description at {user_jd_path}.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "knowledge_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are the Knowledge Agent. Your role is to extract real-world, subjective expectations about how top companies evaluate technical candidates during interviews, focusing on the company: **{company_name}**.\n",
    "Use the `tavily_search` tool to search public internet sources (Reddit, Glassdoor, Blind, Medium, etc.).\n",
    "\n",
    "**Search Strategy**:\n",
    "Construct multiple web queries using templates like:\n",
    "- `{company_name} coding interview expectations site:reddit.com`\n",
    "- `{company_name} behavioral interview rubric site:glassdoor.com`\n",
    "- `{company_name} system design interview site:blind.com`\n",
    "- `{company_name} coding round tips site:medium.com`\n",
    "- `{company_name} leetcode discussion interview prep`\n",
    "You can dynamically vary these queries.\n",
    "\n",
    "**Goal**:\n",
    "1. Search for public reflections and advice related to **{company_name}**'s interview process.\n",
    "2. Analyze search results to extract subjective behavioral expectations.\n",
    "3. Identify 2-4 rubric-like themes (e.g., 'ownership', 'handling ambiguity').\n",
    "4. Generate 2-4 practical communication tips.\n",
    "5. Cite a source URL for each rubric inference if possible.\n",
    "\n",
    "**Output Format**:\n",
    "Return **only** a JSON object like this:\n",
    "```json\n",
    "{{\n",
    "  \"company\": \"{company_name}\",\n",
    "  \"inferred_rubric\": [\n",
    "    {{\n",
    "      \"theme\": \"<Example Theme>\",\n",
    "      \"evidence\": \"<Supporting evidence from search results>\",\n",
    "      \"discussion_reference\": \"<Source URL>\"\n",
    "    }}\n",
    "    // ... more themes\n",
    "  ],\n",
    "  \"communication_tips\": [\n",
    "    \"<Tip 1>\",\n",
    "    \"<Tip 2>\"\n",
    "    // ... more tips\n",
    "  ]\n",
    "}}\n",
    "```\n",
    "\"\"\"),\n",
    "    (\"human\", \"Generate interview insights for {company_name}. Resume context (optional): {user_resume_text}. JD context (optional): {clean_jd}\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are the Planner Agent, orchestrating interview preparation.\n",
    "**Inputs You Have Access To (implicitly via state or tools)**:\n",
    "- Parsed Resume Context (from ../faiss/resume_embeddings )\n",
    "- Parsed Job Description Context (from ../faiss/jd_embeddings )\n",
    "- Knowledge Agent Output: `inferred_rubric` and `communication_tips` for `{company_name}`.\n",
    "- User Preference: `preferred_question_type` (e.g., 'technical', 'behavioral').\n",
    "\n",
    "**Your Responsibilities**:\n",
    "1.  **Synthesize**: Briefly analyze the alignment between resume, JD, and company insights.\n",
    "2.  **Generate Study Plan**: Create a concise, actionable study plan (markdown format).\n",
    "3.  **Suggest LeetCode**: Call `company_leetcode_retriever` for `{company_name}`.\n",
    "4.  **Present Insights**: Format and include the `inferred_rubric` and `communication_tips` in your output.\n",
    "5.  **Embed Insights**: Call `generate_embeddings_and_save` to save the combined text of the rubric and tips to `{FAISS_RUBRIC_PATH}`.\n",
    "6.  **Prepare for Question Agent**: Note the `preferred_question_type` for the next step.\n",
    "\n",
    "**Output Format**:\n",
    "Return **only** a JSON object like this:\n",
    "```json\n",
    "{{\n",
    "  \"study_plan\": \"<Markdown formatted study plan>\",\n",
    "  \"suggested_leetcode\": [\"<LeetCode Q1>\", \"<LeetCode Q2>\"],\n",
    "  \"company_insights_display\": {{\n",
    "    \"company\": \"{company_name}\",\n",
    "    \"inferred_rubric\": [ ... ],\n",
    "    \"communication_tips\": [ ... ]\n",
    "  }},\n",
    "  \"embedding_status\": \"<Status message from generate_embeddings_and_save>\",\n",
    "  \"next_action\": \"Proceed to generate a '{preferred_question_type}' question.\"\n",
    "}}\n",
    "\"\"\"\n",
    "    ),\n",
    "    (\"human\", \"Plan the interview prep for {company_name} based on the available context and user preference for a '{preferred_question_type}' question. Knowledge Agent output: {knowledge_output}\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "question_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are the Question Agent. Your goal is to generate a single, relevant, open-ended interview question.\n",
    "**Inputs You Have Access To (implicitly via state or tools)**:\n",
    "- Question Type Requested: `{preferred_question_type}`\n",
    "- Company: `{company_name}`\n",
    "- Contextual Data (via `retrieve_from_vector_db` from ../faiss/resume_embeddings, ../faiss/jd_embeddings, `{FAISS_RUBRIC_PATH}`)\n",
    "\n",
    "**Your Task**:\n",
    "1. Call `retrieve_from_vector_db` using relevant queries (e.g., job title, key skills, company name, question type) against the resume, JD, and rubric indices to gather context.\n",
    "2. Synthesize the retrieved context.\n",
    "3. Generate **one** interview question of the type `{preferred_question_type}` that is:\n",
    "    - Tailored to the company (`{company_name}`).\n",
    "    - Relevant to the job description and candidate's likely experience.\n",
    "    - Aligned with the company's inferred rubric/culture (if available).\n",
    "    - Clear, professional, and open-ended.\n",
    "\n",
    "**Output Format**:\n",
    "Return **only** a JSON object like this:\n",
    "```json\n",
    "{{\n",
    "  \"question\": \"<The generated interview question>\"\n",
    "}}\n",
    "```\n",
    "\"\"\"),\n",
    "    (\"human\", \"Generate a '{preferred_question_type}' question for {company_name} based on the available context.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "evaluation_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are EvaluationFeedbackAgent, a senior interview coach.\n",
    "**Given**:\n",
    "- question: `{generated_question}`\n",
    "- candidate_answer: `{candidate_answer}`\n",
    "- company_tag: `{company_name}`\n",
    "- rubric_index_path: `{FAISS_RUBRIC_PATH}`\n",
    "\n",
    "**Do the following**:\n",
    "1. Call `retrieve_rubric_snippets` using the `question` and `company_tag` to get relevant evaluation criteria.\n",
    "2. Call `generate_ideal_answer` for the `question` and `company_tag`.\n",
    "3. Call `rewrite_candidate_answer` for the `question` and `candidate_answer`.\n",
    "4. Call `critique_and_advise` using all inputs.\n",
    "   - The critique must highlight strengths, list missed elements (e.g., complexity, STAR method), suggest improvements, use bullet points, and bold key terms.\n",
    "\n",
    "**Output Format**:\n",
    "Return **only** this JSON:\n",
    "```json\n",
    "{{\n",
    "    \"ideal_answer\": \"<Output from generate_ideal_answer>\",\n",
    "    \"improved_answer\": \"<Output from rewrite_candidate_answer>\",\n",
    "    \"detailed_feedback\": \"<Output from critique_and_advise>\"\n",
    "}}\n",
    "```\n",
    "\"\"\"),\n",
    "    (\"human\", \"Evaluate the answer '{candidate_answer}' for the question '{generated_question}' for company '{company_name}'.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "# Replace the LLM-based preprocess agent with the manual function\n",
    "async def manual_preprocess_agent_node(state: InterviewState, config: dict):\n",
    "    print(\"\\nüîß Running preprocess agent manually...\")\n",
    "    resume_path = state[\"user_resume_path\"]\n",
    "    jd_path = state[\"user_jd_path\"]\n",
    "\n",
    "    resume_text = extract_text_with_ocr.invoke(resume_path)\n",
    "    jd_text = extract_text_with_ocr.invoke(jd_path)\n",
    "\n",
    "    clean_jd = re.sub(r\"[^\\S\\r\\n]+\", \" \", jd_text.strip())\n",
    "    clean_jd = re.sub(r\"(?m)^[=_\\\\-]{3,}$\", \"\", clean_jd)\n",
    "\n",
    "    company_match = re.search(r\"(?:at|join)\\\\s+([A-Z][a-zA-Z0-9& ]+)\", clean_jd)\n",
    "    company_name = company_match.group(1).strip() if company_match else \"Unknown\"\n",
    "\n",
    "    generate_embeddings_and_save.invoke({\"text\": resume_text, \"index_path\": FAISS_RESUME_PATH})\n",
    "    generate_embeddings_and_save.invoke({\"text\": clean_jd, \"index_path\": FAISS_JD_PATH})\n",
    "\n",
    "    state[\"user_resume_text\"] = resume_text\n",
    "    state[\"user_jd_text\"] = jd_text\n",
    "    state[\"clean_jd\"] = clean_jd\n",
    "    state[\"company_name\"] = company_name\n",
    "\n",
    "    print(f\"‚úÖ Preprocess Complete. Extracted company: {company_name}\")\n",
    "    return {\"messages\": []}\n",
    "\n",
    "\n",
    "# --- Agent Nodes ---\n",
    "# Helper to create agent nodes\n",
    "async def preprocess_agent_node(state: InterviewState, config: dict):\n",
    "    print(\"\\nüîß Running preprocess agent manually...\")\n",
    "\n",
    "    # Extract paths\n",
    "    resume_path = state[\"user_resume_path\"]\n",
    "    jd_path = state[\"user_jd_path\"]\n",
    "\n",
    "    # Extract raw text\n",
    "    resume_text = extract_text_with_ocr.invoke(resume_path)\n",
    "    jd_text = extract_text_with_ocr.invoke(jd_path)\n",
    "\n",
    "    # Simple clean of JD\n",
    "    clean_jd = re.sub(r\"[^\\S\\r\\n]+\", \" \", jd_text.strip())  # Remove excessive whitespace\n",
    "    clean_jd = re.sub(r\"(?m)^[=_\\\\-]{3,}$\", \"\", clean_jd)   # Remove decorative lines\n",
    "\n",
    "    # Try extracting company name (super basic fallback)\n",
    "    company_match = re.search(r\"(?:at|join)\\s+([A-Z][a-zA-Z0-9& ]+)\", clean_jd)\n",
    "    company_name = company_match.group(1).strip() if company_match else \"Unknown\"\n",
    "\n",
    "    # Generate embeddings and save\n",
    "    generate_embeddings_and_save.invoke({\"text\": resume_text, \"index_path\": FAISS_RESUME_PATH})\n",
    "    generate_embeddings_and_save.invoke({\"text\": clean_jd, \"index_path\": FAISS_JD_PATH})\n",
    "\n",
    "    # Update state\n",
    "    state[\"user_resume_text\"] = resume_text\n",
    "    state[\"user_jd_text\"] = jd_text\n",
    "    state[\"clean_jd\"] = clean_jd\n",
    "    state[\"company_name\"] = company_name\n",
    "\n",
    "    print(f\"\\n‚úÖ Preprocess Complete. Extracted company: {company_name}\")\n",
    "    return {\"messages\": []}\n",
    "\n",
    "def create_agent_node(prompt: ChatPromptTemplate, tools: List[Any]):\n",
    "    agent = prompt | llm.bind_tools(tools)\n",
    "    tool_executor = ToolNode(tools)\n",
    "    async def agent_node(state: InterviewState, config: dict):\n",
    "        print(f\"\\nüì§ Sending to {state.get('current_agent')}:\")\n",
    "        print(json.dumps(state, indent=2, default=str))\n",
    "\n",
    "        try:\n",
    "            result = await agent.ainvoke(state, config)\n",
    "            print(f\"\\nüßæ Raw LLM output for agent {state.get('current_agent')}:\")\n",
    "            print(result.content)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error during ainvoke for agent {state.get('current_agent', 'unknown')}: {e}\")\n",
    "            raise\n",
    "        # If tool calls are requested\n",
    "        if isinstance(result, ToolMessage) or (isinstance(result, AIMessage) and result.tool_calls):\n",
    "             # We delegate to the ToolNode\n",
    "            return {\"messages\": [result]}\n",
    "        else:\n",
    "            # If no tool calls, return the result directly\n",
    "            # Attempt to parse JSON output if expected\n",
    "            try:\n",
    "                if isinstance(result.content, str) and result.content.strip().startswith('{'):\n",
    "                    parsed_output = json.loads(result.content)\n",
    "                    # Update state based on agent\n",
    "                    agent_name = state.get('current_agent', 'unknown')\n",
    "                    if agent_name == 'preprocess':\n",
    "                        print(\"‚úÖ Fields in parsed_output:\", list(parsed_output.keys()))\n",
    "                        state['clean_jd'] = parsed_output.get('clean_jd')\n",
    "                        state['user_resume_text'] = parsed_output.get('user_resume_text')  # Add this\n",
    "                        state['user_jd_text'] = parsed_output.get('clean_jd')\n",
    "                        state['company_name'] = parsed_output.get('company_name')\n",
    "                    elif agent_name == 'knowledge':\n",
    "                        state['knowledge_output'] = parsed_output\n",
    "                    elif agent_name == 'planner':\n",
    "                        state['planner_output'] = parsed_output\n",
    "                    elif agent_name == 'question':\n",
    "                        state['generated_question'] = parsed_output.get('question')\n",
    "                    elif agent_name == 'evaluate':\n",
    "                        state['evaluation_output'] = parsed_output\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: Could not parse JSON output from {state.get('current_agent', 'agent')}\")\n",
    "                # Fallback or handle error as needed\n",
    "            return {\"messages\": [result]}\n",
    "    return agent_node, tool_executor\n",
    "\n",
    "# Create nodes\n",
    "preprocess_tools = [extract_text_with_ocr, generate_embeddings_and_save]\n",
    "preprocess_agent_node, preprocess_tool_node = create_agent_node(preprocessing_prompt, preprocess_tools)\n",
    "\n",
    "knowledge_tools = [tavily_tool]\n",
    "knowledge_agent_node, knowledge_tool_node = create_agent_node(knowledge_prompt, knowledge_tools)\n",
    "\n",
    "planner_tools = [company_leetcode_retriever, generate_embeddings_and_save]\n",
    "planner_agent_node, planner_tool_node = create_agent_node(planner_prompt, planner_tools)\n",
    "\n",
    "question_tools = [retrieve_from_vector_db]\n",
    "question_agent_node, question_tool_node = create_agent_node(question_prompt, question_tools)\n",
    "\n",
    "evaluation_tools = [retrieve_rubric_snippets, generate_ideal_answer, rewrite_candidate_answer, critique_and_advise]\n",
    "evaluation_agent_node, evaluation_tool_node = create_agent_node(evaluation_prompt, evaluation_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a761bab",
   "metadata": {},
   "source": [
    "## 4. Graph Definition\n",
    "\n",
    "Define the workflow connecting the agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9cba1e9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Node `preprocess` already present.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m workflow.add_node(\u001b[33m\"\u001b[39m\u001b[33mpreprocess\u001b[39m\u001b[33m\"\u001b[39m, preprocess_agent_node)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Override the node with the manual version\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mworkflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_node\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpreprocess\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmanual_preprocess_agent_node\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# workflow.add_node(\"preprocess_tools\", preprocess_tool_node)\u001b[39;00m\n\u001b[32m      9\u001b[39m workflow.add_node(\u001b[33m\"\u001b[39m\u001b[33mknowledge\u001b[39m\u001b[33m\"\u001b[39m, knowledge_agent_node)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langgraph/graph/state.py:380\u001b[39m, in \u001b[36mStateGraph.add_node\u001b[39m\u001b[34m(self, node, action, metadata, input, retry, destinations)\u001b[39m\n\u001b[32m    378\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nodes:\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNode `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` already present.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m node == END \u001b[38;5;129;01mor\u001b[39;00m node == START:\n\u001b[32m    382\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNode `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` is reserved.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Node `preprocess` already present."
     ]
    }
   ],
   "source": [
    "# --- Graph Construction ---\n",
    "workflow = StateGraph(InterviewState)\n",
    "\n",
    "# # Add nodes for each agent and their tools\n",
    "workflow.add_node(\"preprocess\", preprocess_agent_node)\n",
    "# Override the node with the manual version\n",
    "workflow.add_node(\"preprocess\", manual_preprocess_agent_node)\n",
    "# workflow.add_node(\"preprocess_tools\", preprocess_tool_node)\n",
    "workflow.add_node(\"knowledge\", knowledge_agent_node)\n",
    "workflow.add_node(\"knowledge_tools\", knowledge_tool_node)\n",
    "workflow.add_node(\"planner\", planner_agent_node)\n",
    "workflow.add_node(\"planner_tools\", planner_tool_node)\n",
    "workflow.add_node(\"question\", question_agent_node)\n",
    "workflow.add_node(\"question_tools\", question_tool_node)\n",
    "workflow.add_node(\"record_answer\", record_and_transcribe_audio) # Direct tool call node\n",
    "workflow.add_node(\"evaluate\", evaluation_agent_node)\n",
    "workflow.add_node(\"evaluate_tools\", evaluation_tool_node)\n",
    "\n",
    "# Define edges\n",
    "workflow.set_entry_point(\"preprocess\")\n",
    "\n",
    "# Preprocessing Agent Logic\n",
    "# workflow.add_edge(\"preprocess\", \"preprocess_tools\")\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"preprocess_tools\",\n",
    "#     tools_condition,\n",
    "#     {\"continue\": \"knowledge\", END: END} # If tool call needed, loop back via tools_condition, else go to knowledge\n",
    "# )\n",
    "workflow.add_edge(\"preprocess\", \"knowledge\")\n",
    "\n",
    "# Knowledge Agent Logic\n",
    "workflow.add_edge(\"knowledge\", \"knowledge_tools\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"knowledge_tools\",\n",
    "    tools_condition,\n",
    "    {\"continue\": \"planner\", END: END}\n",
    ")\n",
    "\n",
    "# Planner Agent Logic\n",
    "workflow.add_edge(\"planner\", \"planner_tools\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"planner_tools\",\n",
    "    tools_condition,\n",
    "    {\"continue\": \"question\", END: END}\n",
    ")\n",
    "\n",
    "# Question Agent Logic\n",
    "workflow.add_edge(\"question\", \"question_tools\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"question_tools\",\n",
    "    tools_condition,\n",
    "    {\"continue\": \"record_answer\", END: END} # After question is generated, record answer\n",
    ")\n",
    "\n",
    "# Record Answer Node\n",
    "workflow.add_edge(\"record_answer\", \"evaluate\") # After recording, go to evaluation\n",
    "\n",
    "# Evaluation Agent Logic\n",
    "workflow.add_edge(\"evaluate\", \"evaluate_tools\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"evaluate_tools\",\n",
    "    tools_condition,\n",
    "    {\"continue\": END, END: END} # End after evaluation\n",
    ")\n",
    "\n",
    "# Compile the graph\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "\n",
    "print(\"Graph compiled successfully!\")\n",
    "# Optional: Visualize the graph\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph: {e}. Make sure graphviz and mermaid are installed/configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9011b1",
   "metadata": {},
   "source": [
    "## 5. Execution and Interaction\n",
    "\n",
    "Run the graph with user inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Interview Prep Workflow ---\n",
      "Resume: /Users/sahilpawar/Documents/interview-unlocked/placeholder_resume.pdf\n",
      "Job Description: /Users/sahilpawar/Documents/interview-unlocked/amazon-jd.txt\n",
      "Preferred Question Type: technical\n",
      "------------------------------\n",
      "\n",
      "üì§ Sending to preprocess:\n",
      "{\n",
      "  \"messages\": [],\n",
      "  \"user_resume_path\": \"/Users/sahilpawar/Documents/interview-unlocked/placeholder_resume.pdf\",\n",
      "  \"user_jd_path\": \"/Users/sahilpawar/Documents/interview-unlocked/amazon-jd.txt\",\n",
      "  \"preferred_question_type\": \"technical\",\n",
      "  \"current_agent\": \"preprocess\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßæ Raw LLM output for agent preprocess:\n",
      "\n",
      "\n",
      "--- Finished Agent: preprocess ---\n",
      "------------------------------\n",
      "\n",
      "üì§ Sending to preprocess:\n",
      "{\n",
      "  \"messages\": [\n",
      "    \"content='' additional_kwargs={'function_call': {'name': 'extract_text_with_ocr', 'arguments': '{\\\"file_path\\\": \\\"/Users/sahilpawar/Documents/interview-unlocked/amazon-jd.txt\\\"}'}} response_metadata={'finish_reason': 'STOP', 'safety_ratings': []} id='run-687091e9-ac29-4ebc-bb35-087773aedb95' tool_calls=[{'name': 'extract_text_with_ocr', 'args': {'file_path': '/Users/sahilpawar/Documents/interview-unlocked/placeholder_resume.pdf'}, 'id': '34901b1c-d5b9-40e4-a3d5-6ca9793b5435', 'type': 'tool_call'}, {'name': 'extract_text_with_ocr', 'args': {'file_path': '/Users/sahilpawar/Documents/interview-unlocked/amazon-jd.txt'}, 'id': 'decbb00e-a77e-4759-bb4d-dd4cbe7c2ea4', 'type': 'tool_call'}] usage_metadata={'input_tokens': 432, 'output_tokens': 58, 'total_tokens': 490, 'input_token_details': {'cache_read': 0}}\"\n",
      "  ],\n",
      "  \"user_resume_path\": \"/Users/sahilpawar/Documents/interview-unlocked/placeholder_resume.pdf\",\n",
      "  \"user_jd_path\": \"/Users/sahilpawar/Documents/interview-unlocked/amazon-jd.txt\",\n",
      "  \"preferred_question_type\": \"technical\",\n",
      "  \"current_agent\": \"preprocess\"\n",
      "}\n",
      "\n",
      "‚ùå Error during ainvoke for agent preprocess: \"Input to ChatPromptTemplate is missing variables {'user_resume_text', 'company_name', 'clean_jd'}.  Expected: ['clean_jd', 'company_name', 'messages', 'user_resume_text'] Received: ['messages', 'user_resume_path', 'user_jd_path', 'preferred_question_type', 'current_agent']\\nNote: if you intended {user_resume_text} to be part of the string and not a variable, please escape it with double curly braces like: '{{user_resume_text}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
      "\n",
      "--- Finished Agent: knowledge ---\n",
      "------------------------------\n",
      "\n",
      "An error occurred during graph execution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/7c/8ggzy01x6f99z_bwb4_ttym40000gn/T/ipykernel_26994/2162888431.py\", line 113, in <module>\n",
      "    await run_graph()\n",
      "  File \"/var/folders/7c/8ggzy01x6f99z_bwb4_ttym40000gn/T/ipykernel_26994/2162888431.py\", line 44, in run_graph\n",
      "    async for event in graph.astream_events(initial_state, config, version=\"v1\"):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 1394, in astream_events\n",
      "    async for event in event_stream:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/tracers/event_stream.py\", line 790, in _astream_events_implementation_v1\n",
      "    async for log in _astream_log_implementation(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/tracers/log_stream.py\", line 702, in _astream_log_implementation\n",
      "    await task\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/tracers/log_stream.py\", line 654, in consume_astream\n",
      "    async for chunk in runnable.astream(input, config, **kwargs):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 2676, in astream\n",
      "    async for _ in runner.atick(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/tracers/log_stream.py\", line 279, in tap_output_aiter\n",
      "    async for chunk in output:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 1461, in atransform\n",
      "    async for ichunk in input:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 1024, in astream\n",
      "    yield await self.ainvoke(input, config, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langgraph/utils/runnable.py\", line 439, in ainvoke\n",
      "    ret = await self.afunc(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/7c/8ggzy01x6f99z_bwb4_ttym40000gn/T/ipykernel_26994/3708857086.py\", line 230, in agent_node\n",
      "    result = await agent.ainvoke(state, config)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3075, in ainvoke\n",
      "    input = await coro_with_context(part(), context, create_task=True)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/prompts/base.py\", line 242, in ainvoke\n",
      "    return await self._acall_with_config(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 1981, in _acall_with_config\n",
      "    output: Output = await coro_with_context(coro, context)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/prompts/base.py\", line 195, in _aformat_prompt_with_error_handling\n",
      "    _inner_input = self._validate_input(inner_input)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/prompts/base.py\", line 183, in _validate_input\n",
      "    raise KeyError(\n",
      "KeyError: \"Input to ChatPromptTemplate is missing variables {'user_resume_text', 'company_name', 'clean_jd'}.  Expected: ['clean_jd', 'company_name', 'messages', 'user_resume_text'] Received: ['messages', 'user_resume_path', 'user_jd_path', 'preferred_question_type', 'current_agent']\\nNote: if you intended {user_resume_text} to be part of the string and not a variable, please escape it with double curly braces like: '{{user_resume_text}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"\n",
      "During task with name 'knowledge' and id '04178219-6561-a158-6311-24dcdb82435f'\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import pprint\n",
    "\n",
    "# --- Execution ---\n",
    "\n",
    "# IMPORTANT: Set the path to your Google Cloud credentials file\n",
    "# This is needed for the record_and_transcribe_audio tool\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"key.json\" # Replace with the actual path to your key.json\n",
    "\n",
    "# --- User Inputs ---\n",
    "# !!! IMPORTANT: Replace these with the actual paths to your files !!!\n",
    "resume_file = os.path.join(os.getcwd(), \"placeholder_resume.pdf\") # e.g., \"/path/to/your/resume.pdf\"\n",
    "jd_file = os.path.join(os.getcwd(), \"amazon-jd.txt\") #\"/path/to/your/job_description.txt\"\n",
    "user_preferred_question_type = \"technical\" # Options: \"technical\", \"behavioral\", \"system design\", \"debugging/problem-solving\"\n",
    "\n",
    "# Create dummy files if they don't exist for the example run\n",
    "if not os.path.exists(resume_file):\n",
    "    with open(resume_file, \"w\") as f:\n",
    "        f.write(\"Sample Resume Content: Python Developer with 5 years experience in web development and data analysis.\")\n",
    "if not os.path.exists(jd_file):\n",
    "     with open(jd_file, \"w\") as f:\n",
    "        f.write(\"Sample Job Description: Looking for a Senior Software Engineer at Google. Requires strong Python skills, experience with distributed systems, and cloud platforms.\")\n",
    "\n",
    "# Define the initial state to start the graph\n",
    "initial_state = {\n",
    "    \"messages\": [],\n",
    "    \"user_resume_path\": resume_file,\n",
    "    \"user_jd_path\": jd_file,\n",
    "    \"preferred_question_type\": user_preferred_question_type,\n",
    "    \"current_agent\": \"preprocess\" # Start with the preprocessing agent\n",
    "}\n",
    "\n",
    "# Configuration for the graph run (e.g., unique thread ID)\n",
    "config = {\"configurable\": {\"thread_id\": \"interview-prep-thread-1\"}}\n",
    "\n",
    "async def run_graph():\n",
    "    final_state = None\n",
    "    print(\"--- Starting Interview Prep Workflow ---\")\n",
    "    print(f\"Resume: {resume_file}\")\n",
    "    print(f\"Job Description: {jd_file}\")\n",
    "    print(f\"Preferred Question Type: {user_preferred_question_type}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    async for event in graph.astream_events(initial_state, config, version=\"v1\"):\n",
    "        kind = event[\"event\"]\n",
    "        tags = event.get(\"tags\", [])\n",
    "        if kind == \"on_chat_model_stream\":\n",
    "            content = event[\"data\"][\"chunk\"].content\n",
    "            if content:\n",
    "                # Print LLM tokens as they arrive\n",
    "                print(content, end=\"|\")\n",
    "        elif kind == \"on_tool_start\":\n",
    "            print(f\"\\n--- Calling Tool: {event['name']} ---\")\n",
    "            print(f\"   Args: {event['data'].get('input')}\")\n",
    "        elif kind == \"on_tool_end\":\n",
    "            print(f\"--- Tool Result: {event['name']} ---\")\n",
    "            print(f\"   Output: {event['data'].get('output')}\")\n",
    "            print(\"-\" * 30)\n",
    "        elif kind == \"on_chain_end\":\n",
    "             # Check if it's the end of a specific agent node run\n",
    "            if event[\"name\"] in [\"preprocess\", \"knowledge\", \"planner\", \"question\", \"evaluate\"]:\n",
    "                 print(f\"\\n--- Finished Agent: {event['name']} ---\")\n",
    "                 # pprint.pprint(event['data'].get('output'), indent=2) # Print agent output if needed\n",
    "                 print(\"-\" * 30)\n",
    "\n",
    "\n",
    "        # Track the final state\n",
    "        if kind == \"on_graph_end\":\n",
    "            final_state = event['data']['output']\n",
    "\n",
    "\n",
    "    print(\"\\n--- Workflow Complete ---\")\n",
    "\n",
    "    if final_state:\n",
    "        print(\"\\n--- Final Results ---\")\n",
    "        # Extract and print key information from the final state\n",
    "        planner_output = final_state.get('planner_output', {})\n",
    "        evaluation_output = final_state.get('evaluation_output', {})\n",
    "\n",
    "        print(\"\\n**Study Plan:**\")\n",
    "        print(planner_output.get('study_plan', 'Not generated.'))\n",
    "\n",
    "        print(\"\\n**Suggested LeetCode:**\")\n",
    "        pprint.pprint(planner_output.get('suggested_leetcode', 'Not generated.'))\n",
    "\n",
    "        print(\"\\n**Company Insights:**\")\n",
    "        pprint.pprint(planner_output.get('company_insights_display', 'Not generated.'))\n",
    "\n",
    "        print(f\"\\n**Generated Question ({final_state.get('preferred_question_type', 'N/A')}):**\")\n",
    "        print(final_state.get('generated_question', 'Not generated.'))\n",
    "\n",
    "        print(\"\\n**Your Transcribed Answer:**\")\n",
    "        # The actual transcribed answer isn't directly stored in the state by the tool node,\n",
    "        # but it was passed to the evaluation agent. We print the placeholder for clarity.\n",
    "        # In a real UI, you'd capture the output of the 'record_answer' node.\n",
    "        print(final_state.get('candidate_answer', '[Answer was recorded and passed to evaluation]'))\n",
    "\n",
    "\n",
    "        print(\"\\n**Evaluation Feedback:**\")\n",
    "        print(\"\\n*Ideal Answer (Placeholder):*\")\n",
    "        print(evaluation_output.get('ideal_answer', 'Not generated.'))\n",
    "        print(\"\\n*Improved Answer (Placeholder):*\")\n",
    "        print(evaluation_output.get('improved_answer', 'Not generated.'))\n",
    "        print(\"\\n*Detailed Feedback (Placeholder):*\")\n",
    "        print(evaluation_output.get('detailed_feedback', 'Not generated.'))\n",
    "    else:\n",
    "        print(\"Workflow did not complete successfully or final state not captured.\")\n",
    "\n",
    "# Run the asynchronous function\n",
    "import traceback\n",
    "\n",
    "try:\n",
    "    await run_graph()\n",
    "except Exception as e:\n",
    "    print(\"\\nAn error occurred during graph execution:\")\n",
    "    traceback.print_exc()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
