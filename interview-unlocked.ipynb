{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fd836ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745126316.690518 13948258 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -qU langchain langgraph langgraph-swarm langchain-google-genai google-generativeai langchain_community faiss-cpu tavily-python google-cloud-speech sounddevice scipy pdfminer.six python-dotenv langchain-openai numpy pandas pytesseract pdf2image openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d0259e",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Install necessary libraries and import required modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262ab74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "#%pip install -qU langchain langgraph langgraph-swarm langchain-google-genai langchain_community faiss-cpu tavily-python google-cloud-speech sounddevice scipy pdfminer.six python-dotenv langchain-openai\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import uuid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sounddevice as sd\n",
    "import scipy.io.wavfile as wav\n",
    "from typing import List, Dict, Any, Optional, TypedDict\n",
    "\n",
    "# Replace Ollama with Google Generative AI (Gemini)\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "# from langchain_ollama.chat_models import ChatOllama\n",
    "# from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "# Use pydantic.v1 for compatibility as suggested by the warning\n",
    "from pydantic.v1 import BaseModel, Field \n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "# Removed unused langgraph_swarm imports\n",
    "\n",
    "from google.cloud import speech\n",
    "from pdfminer.high_level import extract_text\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (for API keys like Tavily, Google Cloud)\n",
    "load_dotenv()\n",
    "\n",
    "# --- Configuration ---\n",
    "FAISS_RESUME_PATH = \"./faiss/resume_embeddings\"\n",
    "FAISS_JD_PATH = \"./faiss/jd_embeddings\"\n",
    "FAISS_RUBRIC_PATH = \"./faiss/rubric_embeddings\"\n",
    "FAISS_KNOWLEDGE_PATH = \"./faiss/knowledge_embeddings\"\n",
    "os.makedirs(os.path.dirname(FAISS_RESUME_PATH), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(FAISS_JD_PATH), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(FAISS_RUBRIC_PATH), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(FAISS_KNOWLEDGE_PATH), exist_ok=True)\n",
    "# Retrieve the API key loaded by load_dotenv()\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Initialize LLM (using Gemini)\n",
    "# Pass the API key explicitly\n",
    "# import google.generativeai as genai\n",
    "# genai.configure(api_key=google_api_key)\n",
    "# models = genai.list_models()\n",
    "\n",
    "# for m in models:\n",
    "#     print(m.name, m.supported_generation_methods)\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",  # Must be valid in your project\n",
    "    temperature=0.1,\n",
    "    convert_system_message_to_human=True,\n",
    "    google_api_key=google_api_key  # from .env\n",
    ")\n",
    "# llm = ChatOllama(model=\"llama3\", temperature=0.1)\n",
    "\n",
    "# Initialize Embeddings Model (using Gemini)\n",
    "# Pass the API key explicitly\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=google_api_key\n",
    ")\n",
    "# embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# Initialize Checkpointer\n",
    "memory = InMemorySaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3adb8",
   "metadata": {},
   "source": [
    "Okay, I will update the interview-unlocked.ipynb notebook to implement the agentic workflow described in design.md using `langgraph` and `langgraph_swarm`.\n",
    "\n",
    "Made changes.\n",
    "\n",
    "OK. I have populated the interview-unlocked.ipynb notebook with the Python code to implement the multi-agent interview preparation system based on your design.md file and using LangGraph.\n",
    "\n",
    "The notebook now contains the following sections:\n",
    "\n",
    "1.  **Setup and Imports:** Installs necessary packages (commented out by default) and imports all required libraries and modules. It also sets up configurations like FAISS paths and initializes the LLM and embedding models.\n",
    "2.  **Tool Definitions:** Implements all the tools required by the agents, such as text extraction, embedding generation, vector retrieval, web search, LeetCode retrieval (placeholder), audio recording/transcription, and evaluation helper tools (placeholders).\n",
    "3.  **Agent Definitions:** Defines the `InterviewState` TypedDict to manage the data flow between agents. It includes the prompts for each agent (Preprocessing, Knowledge, Planner, Question, Evaluation) derived from design.md and sets up the agent nodes using LangGraph's `ToolNode` and a helper function.\n",
    "4.  **Graph Definition:** Constructs the `StateGraph`, adds nodes for each agent and its tools, defines the sequential edges connecting them (Preprocessing -> Knowledge -> Planner -> Question -> Record Answer -> Evaluate -> END), and compiles the graph. It also includes code to optionally visualize the graph structure.\n",
    "5.  **Execution and Interaction:** Provides an example of how to run the graph. You'll need to **modify the `resume_file` and `jd_file` variables** to point to your actual resume and job description files. It then runs the graph asynchronously, streams the events (like tool calls and LLM outputs), and finally prints the key outputs extracted from the final state (study plan, suggested LeetCode, generated question, transcribed answer, and evaluation feedback).\n",
    "\n",
    "Please review the notebook, especially the placeholder file paths in the \"Execution and Interaction\" section, and ensure your environment (API keys, Ollama server if used, Google Cloud credentials) is set up correctly before running the cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf2dadb",
   "metadata": {},
   "source": [
    "# Interview Unlocked: Agentic Interview Preparation System\n",
    "\n",
    "This notebook implements a multi-agent system using LangGraph and LangGraph Swarm to help users prepare for job interviews. It follows the design outlined in `design.md`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "362b2961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw result structure: dict_keys(['title', 'url', 'content', 'score'])\n",
      "Amazon System Design Interview Questions: Top Examples & Tips ...: To effectively communicate during interviews, check out Amazon behavioral interview questions.\n",
      "\n",
      "Conclusion\n",
      "\n",
      "The Amazon system design interview is an important gatekeeper for any role involving large-scale systems at the company. By understanding the essential system design concepts and practicing key interview questions, you’ll be well-equipped to tackle whatever problem is thrown your way. [...] In this article, we'll discuss what Amazon’s system design interview process entails, go deeper into essential system design concepts, explore top system design questions at Amazon (with example scenarios), and offer tips for answering them effectively.\n",
      "\n",
      "Understanding the Amazon System Design Interview Process\n",
      "\n",
      "The Amazon system design interview is a crucial part of the hiring process for engineers at Amazon. [...] Amazon interviewers are looking to evaluate several key aspects of your capabilities during a system design interview. Firstly, they want to see your problem-solving and design skills: can you take an ambiguous, high-level problem and break it down into a sound architectural solution? This means they expect you to cover the major components, discuss data flow, and ensure the design meets the requirements (scalability, reliability, etc.). There’s also a strong emphasis on handling trade-offs – https://www.designgurus.io/blog/system-design-interview-amazon\n",
      "Top Amazon System Design Interview Questions (2024): Q4. What is the Amazon system design interview process like?\n",
      "\n",
      "The Amazon systems design interview happens during the on-site interview, also known as the Loop. At the design interview, you’re typically asked to build an arbitrary system with a given set of parameters. Note that there’s more emphasis on systems design interviews for senior software and managerial positions.\n",
      "\n",
      "Q5. Which concepts are to be learned for the Amazon System Design Interview? [...] Amazon system design interview questions are crucial to the Amazon technical interview. A system design interview analyzes your ability to solve problems and create systems that can solve the company’s or clients’ problems. You may think of it as a brainstorming session where you’ll be expected to discuss a complex system efficiently. [...] The Loop Interview – Amazon’s on-site interview is known as the Loop. The Loop has 3-5 rounds of interviews – 1-2 coding rounds, 1-2 design rounds, a leadership round (based on Amazon’s leadership principles), and a Bar-raiser round. It is in the on-site design round that you’re asked Amazon systems design interview questions.\n",
      "\n",
      "Also read: Amazon Interview Process Prep Guide\n",
      "\n",
      "Concepts to Prepare for Amazon System Design Interview https://interviewkickstart.com/blogs/interview-questions/amazon-system-design-interview-questions\n",
      "Cracking Amazon System Design Interview: Top Questions and ...: The core purpose of this interview is to assess your approach to designing a system, critical thinking and creativity during design, the reasoning behind choosing components, and your ability to evolve the system based on requirements.\n",
      "\n",
      "What you’ll be asked\n",
      "\n",
      "System Design questions are rather vague and open-ended. They can prompt you to design a range of simple to complex, large-scale systems.\n",
      "\n",
      "Some commonly asked System Design questions at Amazon include: [...] On top of all this, you’ll need to present a solution to the design problem within a specific time, no more than 45 minutes. To hit all the right points in the right timeframe, having a confident strategy (and even a handy framework) is essential.\n",
      "\n",
      "And what’s unique to Amazon is that displaying leadership principles are a priority for them, even in the System Design Interview. You have the opportunity to demonstrate these principles with how you engage in your interview. For example: [...] Each design requires different design choices, depending on the constraints and requirements of the problem. While you get a broad question, you’re expected to ask clarifying questions from your interviewers to fully understand these nuances of your given problem.\n",
      "\n",
      "What is expected from you\n",
      "\n",
      "Amazon’s interviewers don’t expect you to have experience working with large-scale systems. But they do expect you to know System Design fundamentals, and to leverage them to design a scalable system. https://dev.to/fahimulhaq/cracking-amazon-system-design-interview-top-questions-and-answer-45i1\n",
      "Cracking Amazon System Design Interview: Top Questions and ...: The core purpose of this interview is to assess your approach to designing a system, critical thinking and creativity during design, the reasoning behind choosing components, and your ability to evolve the system based on requirements.\n",
      "\n",
      "What you’ll be asked\n",
      "\n",
      "System Design questions are rather vague and open-ended. They can prompt you to design a range of simple to complex, large-scale systems.\n",
      "\n",
      "Some commonly asked System Design questions at Amazon include: [...] On top of all this, you’ll need to present a solution to the design problem within a specific time, no more than 45 minutes. To hit all the right points in the right timeframe, having a confident strategy (and even a handy framework) is essential.\n",
      "\n",
      "And what’s unique to Amazon is that displaying leadership principles are a priority for them, even in the System Design Interview. You have the opportunity to demonstrate these principles with how you engage in your interview. For example: [...] Each design requires different design choices, depending on the constraints and requirements of the problem. While you get a broad question, you’re expected to ask clarifying questions from your interviewers to fully understand these nuances of your given problem.\n",
      "\n",
      "What is expected from you\n",
      "\n",
      "Amazon’s interviewers don’t expect you to have experience working with large-scale systems. But they do expect you to know System Design fundamentals, and to leverage them to design a scalable system. https://grokkingtechinterview.com/cracking-amazon-system-design-interview-top-questions-and-answer-3df280488203\n",
      "Amazon System Design Interview Questions | GeeksforGeeks: How to Approach Amazon System Design Questions?\n",
      "\n",
      "When tackling system design questions in an Amazon interview, follow a structured approach to demonstrate your ability to design scalable, reliable, and efficient systems. Here’s a step-by-step guide:\n",
      "\n",
      "Step 1. Understand the Problem Statement\n",
      "\n",
      "Step 2. Design the System at a High Level\n",
      "\n",
      "Step 3. Dive into Detailed Design\n",
      "\n",
      "Step 4. Consider Scalability\n",
      "\n",
      "Step 5. Address Reliability and Fault Tolerance\n",
      "\n",
      "Step 6. Discuss Trade-offs [...] Amazon system design interviews feature challenges like creating scalable platforms, optimizing performance, and managing large-scale systems, testing candidates' skills in building robust and efficient architectures. Below are some main questions that have been asked in amazon system design interviews.\n",
      "\n",
      "Q 1. Design a Content Delivery Network (CDN)\n",
      "\n",
      "Develop a CDN architecture to efficiently deliver web content to users across the globe, minimizing latency and improving load times. [...] Amazon System Design Interview Questions\n",
      "\n",
      "Amazon’s system design interviews can be tough, focusing on how well you can build scalable and efficient systems. In this guide, we’ll cover the types of questions you might face, how to approach them, and tips to help you shine. Whether you’re experienced or new to the field, understanding these questions and preparing effectively will help you show off your skills and meet Amazon’s high expectations.\n",
      "\n",
      "Table of Content https://www.geeksforgeeks.org/amazon-system-design-interview-questions/\n",
      "SDE II Interview Prep - Amazon.jobs: You’ll have the chance to discuss your experiences and expertise in several areas that help us determine success at Amazon.\n",
      "\n",
      "These areas include both technical competencies and non-technical competencies that are based off of our Leadership Principles, which different interviewers will be assigned to evaluate.\n",
      "\n",
      "Download video transcript\n",
      "\n",
      "System design\n",
      "\n",
      "At Amazon, designing software systems is unique due to our size and speed of change. Expect at least one question on software systems design. [...] You’ll have 90 minutes to complete two technical questions followed by 20 minutes of Systems Design scenarios and an 8-minute multiple choice Work Style Survey related to our Leadership Principles.\n",
      "\n",
      "Learn more about the Online Assessment.\n",
      "\n",
      "Video: What is Amazon assessing?\n",
      "\n",
      "Interview loop\n",
      "\n",
      "Your loop will include four 55-minute interviews where you’ll meet with members of our software development community. [...] Your interviewer will ask questions related to your design, and you should ask questions to complete and validate your design.\n",
      "\n",
      "Objectives\n",
      "\n",
      "System design resources\n",
      "\n",
      "System design videos\n",
      "\n",
      "SDE Interview FAQs\n",
      "\n",
      "Watch our team members from around the world answer seven of the most frequently asked questions from SDE candidates!\n",
      "\n",
      "System Design Interview Prep\n",
      "\n",
      "Learn how to approach, analyze, and solve technical questions in your interview.\n",
      "\n",
      "High-Level Design https://amazon.jobs/content/en/how-we-hire/sde-ii-interview-prep\n",
      "System Design Interview (Amazon) - Interviewing.io: Please read our definitive guide on Amazon's hiring process and questions\n",
      "\n",
      "Design LeetCode - A System Design Interview with an Amazon engineer\n",
      "\n",
      "Watch someone solve the design leetcode problem in an interview with an Amazon engineer and see the feedback their interviewer left them. Explore this problem and others in our library of interview replays.\n",
      "\n",
      "Interview Summary\n",
      "\n",
      "Problem type\n",
      "\n",
      "Design LeetCode\n",
      "\n",
      "Interview question [...] Design a coding competition platform with a leaderboard and execution environment.\n",
      "\n",
      "Read more about the questions\n",
      "\n",
      "Interview Feedback\n",
      "\n",
      "Feedback about Electric Tetrahedron (the interviewee)\n",
      "\n",
      "Feedback about Metal Cephalopod (the interviewer)\n",
      "\n",
      "Interview Transcript\n",
      "\n",
      "We know exactly what to do and say to get the company, title, and salary you want.\n",
      "\n",
      "Interview prep and job hunting are chaos and pain. We can help. Really. https://interviewing.io/mocks/amazon-system-design-design-leetcode\n",
      "\"Design LinkedIn\" - System design mock with Senior SWE at Amazon: Today's system design mock interview: \"Design YouTube.\" Candidate: Shivali, current Senior software engineer at Amazon and a coach on our https://www.youtube.com/watch?v=ICu8g9auh8E\n",
      "11 Most-Asked System Design Interview Questions (+ answers): Discuss the product catalog and inventory management system, including categories, attributes, pricing, and availability.\n",
      "Address the search functionality, including features like auto-suggestions, filters, and sorting options.\n",
      "Discuss the shopping cart and checkout process, considering guest checkout, saved carts, and various payment options.\n",
      "Time permitting, consider the order management system, including order tracking, notifications, and returns/refunds handling. https://igotanoffer.com/blogs/tech/system-design-interviews\n",
      "The System Design Interview, 2nd Edition - Amazon.com: A comprehensive book that provides the necessary knowledge, concepts, and skills to pass your system design interview. https://www.amazon.com/System-Design-Interview-2nd/dp/B09559NJKL\n",
      "What is the best way to prepare for a System Design interview for ...: Preparing for a System Design interview for Amazon requires a strategic approach and a solid understanding of key concepts. https://www.quora.com/What-is-the-best-way-to-prepare-for-a-System-Design-interview-for-Amazon\n",
      "Amazon SDE final interviews -- will I get some system design? - Reddit: Depends on what position you are getting interviewed for, If you are interviewing for sde2 and above you will have system design. Upvote 1 https://www.reddit.com/r/leetcode/comments/1ghe9zs/amazon_sde_final_interviews_will_i_get_some/\n",
      "System Design Interview – An insider's guide: Xu, Alex - Amazon.com: This book provides a step-by-step framework for how to tackle a system design question. It includes many real-world examples to illustrate the systematic https://www.amazon.com/System-Design-Interview-insiders-Second/dp/B08CMF2CQF\n",
      "Amazon | SDE2 | System Design Question - LeetCode Discuss: Recently I have attended the Amazon Interview Process but got rejected in System Design round. Question was like: There is an existing food https://leetcode.com/discuss/interview-question/2150819/amazon-sde2-system-design-question\n",
      "Amazon system design mock interview (with Senior SWE) - YouTube: GET 1-to-1 COACHING for system design interviews: https://www.youtube.com/watch?v=i_RCwKflp3I&pp=ygUTI2FtYXpvbmRlc2lnbnN5c3RlbQ%3D%3D\n"
     ]
    }
   ],
   "source": [
    "# --- Tool Implementations ---\n",
    "\n",
    "@tool\n",
    "def extract_text_with_ocr(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from a file. Uses pytesseract OCR for PDF files,\n",
    "    otherwise reads as plain text.\n",
    "    Requires Tesseract OCR engine to be installed and pdf2image library.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if file_path.lower().endswith('.pdf'):\n",
    "            print(f\"Extracting text from PDF using OCR: {file_path}\")\n",
    "            # Check if Tesseract is installed and accessible\n",
    "            try:\n",
    "                pytesseract.get_tesseract_version()\n",
    "            except Exception as e:\n",
    "                 return f\"Tesseract is not installed or not in your PATH: {e}\"\n",
    "\n",
    "            try:\n",
    "                images = convert_from_path(file_path)\n",
    "                full_text = \"\"\n",
    "                for i, image in enumerate(images):\n",
    "                    print(f\"Processing page {i+1}/{len(images)}...\")\n",
    "                    # Use pytesseract to do OCR on the image\n",
    "                    text = pytesseract.image_to_string(image)\n",
    "                    full_text += text + \"\\n\" # Add newline between pages\n",
    "                print(\"PDF OCR finished.\")\n",
    "                return full_text.strip()\n",
    "            except Exception as e:\n",
    "                return f\"Error during PDF processing or OCR: {e}\"\n",
    "        else:\n",
    "            # Handle non-PDF files as plain text\n",
    "            print(f\"Reading text file: {file_path}\")\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                return f.read()\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: File not found at {file_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"An unexpected error occurred while processing {file_path}: {e}\"\n",
    "\n",
    "@tool\n",
    "def generate_resume_embeddings_and_save(text: str) -> str:\n",
    "    \"\"\"Generates embeddings for the Job Description text and saves/updates the FAISS JD index.\"\"\"\n",
    "    index_path = FAISS_RESUME_PATH # Use the specific path\n",
    "    try:\n",
    "        texts = [text] # FAISS expects a list\n",
    "        if os.path.exists(index_path):\n",
    "            vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "            vectorstore.add_texts(texts)\n",
    "        else:\n",
    "            vectorstore = FAISS.from_texts(texts, embeddings)\n",
    "        vectorstore.save_local(index_path)\n",
    "        return f\"JD Embeddings generated and saved to {index_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error generating/saving JD embeddings: {e}\"\n",
    "\n",
    "@tool\n",
    "def generate_jd_embeddings_and_save(text: str) -> str:\n",
    "    \"\"\"Generates embeddings for the Job Description text and saves/updates the FAISS JD index.\"\"\"\n",
    "    index_path = FAISS_JD_PATH # Use the specific path\n",
    "    try:\n",
    "        texts = [text] # FAISS expects a list\n",
    "        if os.path.exists(index_path):\n",
    "            vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "            vectorstore.add_texts(texts)\n",
    "        else:\n",
    "            vectorstore = FAISS.from_texts(texts, embeddings)\n",
    "        vectorstore.save_local(index_path)\n",
    "        return f\"JD Embeddings generated and saved to {index_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error generating/saving JD embeddings: {e}\"\n",
    "\n",
    "@tool\n",
    "def generate_rubric_embeddings_and_save(text: str) -> str:\n",
    "    \"\"\"Generates embeddings for the Job Description text and saves/updates the FAISS JD index.\"\"\"\n",
    "    index_path = FAISS_RUBRIC_PATH # Use the specific path\n",
    "    try:\n",
    "        texts = [text] # FAISS expects a list\n",
    "        if os.path.exists(index_path):\n",
    "            vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "            vectorstore.add_texts(texts)\n",
    "        else:\n",
    "            vectorstore = FAISS.from_texts(texts, embeddings)\n",
    "        vectorstore.save_local(index_path)\n",
    "        return f\"JD Embeddings generated and saved to {index_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error generating/saving JD embeddings: {e}\"\n",
    "\n",
    "@tool\n",
    "def retrieve_resume_embeddings_from_vector_db(query: str, k: int = 3) -> List[str]:\n",
    "    \"\"\"Retrieves relevant documents from the resume FAISS index.\"\"\"\n",
    "    try:\n",
    "        index_path = FAISS_RESUME_PATH\n",
    "        if not os.path.exists(index_path):\n",
    "            return [\"Resume vector index not found.\"]\n",
    "        vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "        results = vectorstore.similarity_search(query, k=k)\n",
    "        return [doc.page_content for doc in results]\n",
    "    except Exception as e:\n",
    "        return [f\"Error retrieving from resume vector DB: {e}\"]\n",
    "    \n",
    "@tool\n",
    "def retrieve_jd_embeddings_from_vector_db(query: str, k: int = 3) -> List[str]:\n",
    "    \"\"\"Retrieves relevant documents from the resume FAISS index.\"\"\"\n",
    "    try:\n",
    "        index_path = FAISS_JD_PATH\n",
    "        if not os.path.exists(index_path):\n",
    "            return [\"Resume vector index not found.\"]\n",
    "        vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "        results = vectorstore.similarity_search(query, k=k)\n",
    "        return [doc.page_content for doc in results]\n",
    "    except Exception as e:\n",
    "        return [f\"Error retrieving from resume vector DB: {e}\"]\n",
    "\n",
    "@tool\n",
    "def retrieve_rubric_embeddings_from_vector_db(query: str, k: int = 3) -> List[str]:\n",
    "    \"\"\"Retrieves relevant documents from the resume FAISS index.\"\"\"\n",
    "    try:\n",
    "        index_path = FAISS_RUBRIC_PATH\n",
    "        if not os.path.exists(index_path):\n",
    "            return [\"Resume vector index not found.\"]\n",
    "        vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "        results = vectorstore.similarity_search(query, k=k)\n",
    "        return [doc.page_content for doc in results]\n",
    "    except Exception as e:\n",
    "        return [f\"Error retrieving from resume vector DB: {e}\"]\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the API key from environment variables\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "# Check if the API key was loaded\n",
    "if not tavily_api_key:\n",
    "    raise ValueError(\"TAVILY_API_KEY not found in environment variables. Please ensure it is set in your .env file.\")\n",
    "\n",
    "# Tavily Search Tool (already integrated in LangChain)\n",
    "# Pass the API key during initialization\n",
    "tavily_tool = TavilySearchResults(\n",
    "   # name=\"websearcher_tool\",\n",
    "    description=(\n",
    "        \"Use this tool to search the web for real-world interview experiences, coding round expectations, \"\n",
    "        \"and evaluation rubrics for specific companies. Focus on results mainly from Leetcode discussions, Reddit, Glassdoor and stackoverflow\"\n",
    "        \"Your goal is to find behavioral insights — not code solutions — that reveal what\"\n",
    "        \"companies look for in terms of communication, reasoning, and soft skills during interviews.\"\n",
    "    ),\n",
    "    tavily_api_key=tavily_api_key,\n",
    "    max_results=20\n",
    ")\n",
    "results = tavily_tool.invoke(\"Amazon system design interview \")\n",
    "\n",
    "@tool\n",
    "def company_leetcode_problem_retriever(company: str, role_keywords: Optional[List[str]] = None) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieves suggested LeetCode questions for a specific company by reading\n",
    "    from the './Leetcode-company-problem-set.xlsx' file. Each company's\n",
    "    questions are expected to be in a sheet named after the company (case-insensitive).\n",
    "    Questions are assumed to be listed in the first column (A) starting from the first row (A1).\n",
    "    The role_keywords parameter is currently unused but available for future filtering.\n",
    "    \"\"\"\n",
    "    excel_path = './Leetcode-company-problem-set.xlsx'\n",
    "    default_questions = [\"Reverse Linked List\", \"Valid Parentheses\", \"Coin Change\"] # Default if company not found\n",
    "\n",
    "    print(f\"Fetching LeetCode questions for {company} from {excel_path}...\")\n",
    "\n",
    "    try:\n",
    "        # Check if file exists first\n",
    "        if not os.path.exists(excel_path):\n",
    "            print(f\"Error: Excel file not found at {excel_path}. Returning default questions.\")\n",
    "            return default_questions\n",
    "\n",
    "        # Read all sheet names first to handle case-insensitivity\n",
    "        xls = pd.ExcelFile(excel_path)\n",
    "        sheet_names = xls.sheet_names\n",
    "        target_sheet = None\n",
    "        for name in sheet_names:\n",
    "            if name.lower() == company.lower():\n",
    "                target_sheet = name\n",
    "                break\n",
    "\n",
    "        if target_sheet:\n",
    "            # Read the specific sheet, assuming no header and questions start at A1 (index 0)\n",
    "            df = pd.read_excel(excel_path, sheet_name=target_sheet, header=None)\n",
    "\n",
    "            if not df.empty and df.shape[1] > 0: # Check if dataframe is not empty and has at least one column\n",
    "                 # Questions are in the first column (index 0)\n",
    "                questions = df.iloc[:, 0].dropna().astype(str).tolist()\n",
    "                if questions:\n",
    "                    print(f\"Found {len(questions)} questions for {company} in sheet '{target_sheet}'.\")\n",
    "                    return questions\n",
    "                else:\n",
    "                    print(f\"Sheet '{target_sheet}' for {company} found, but the first column is empty or contains only NaN values.\")\n",
    "                    return default_questions\n",
    "            else:\n",
    "                print(f\"Sheet '{target_sheet}' for {company} found but is empty or has no columns.\")\n",
    "                return default_questions\n",
    "        else:\n",
    "            print(f\"No specific sheet found for '{company}'. Returning default questions.\")\n",
    "            return default_questions\n",
    "\n",
    "    except FileNotFoundError: # Should be caught by os.path.exists, but kept for robustness\n",
    "        print(f\"Error: Excel file not found at {excel_path}. Returning default questions.\")\n",
    "        return default_questions\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the Excel file for {company}: {e}. Returning default questions.\")\n",
    "        return default_questions\n",
    "\n",
    "\n",
    "@tool\n",
    "def record_and_transcribe_audio(duration: int = 15, fs: int = 16000) -> str:\n",
    "    \"\"\"Records audio from the microphone for a specified duration and transcribes it using Google Cloud Speech-to-Text.\"\"\"\n",
    "    print(f\"Recording audio for {duration} seconds... Speak now!\")\n",
    "    audio_file = f\"/tmp/interview_answer_{uuid.uuid4()}.wav\"\n",
    "    try:\n",
    "        # Record audio\n",
    "        recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='float32')\n",
    "        sd.wait()\n",
    "        # Convert to int16 and save\n",
    "        recording_int16 = np.int16(recording * 32767)\n",
    "        wav.write(audio_file, fs, recording_int16)\n",
    "        print(\"Audio recorded.\")\n",
    "\n",
    "        # Transcribe audio\n",
    "        print(\"Transcribing audio...\")\n",
    "        client = speech.SpeechClient() # Assumes GOOGLE_APPLICATION_CREDENTIALS is set\n",
    "        with open(audio_file, \"rb\") as f:\n",
    "            content = f.read()\n",
    "        audio = speech.RecognitionAudio(content=content)\n",
    "        config = speech.RecognitionConfig(\n",
    "            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "            sample_rate_hertz=fs,\n",
    "            language_code=\"en-US\",\n",
    "            enable_automatic_punctuation=True\n",
    "        )\n",
    "        response = client.recognize(config=config, audio=audio)\n",
    "        os.remove(audio_file) # Clean up temporary file\n",
    "\n",
    "        if not response.results:\n",
    "            print(\"Transcription failed: No speech detected.\")\n",
    "            return \"[No speech detected]\"\n",
    "\n",
    "        transcript = \" \".join([result.alternatives[0].transcript for result in response.results])\n",
    "        print(f\"Transcription complete: {transcript}\")\n",
    "        return transcript.strip()\n",
    "    except Exception as e:\n",
    "        if os.path.exists(audio_file):\n",
    "            os.remove(audio_file)\n",
    "        error_msg = f\"Error during audio recording or transcription: {e}\"\n",
    "        print(error_msg)\n",
    "        return error_msg\n",
    "\n",
    "# --- Tools for Evaluation Agent (Simulated - LLM will act based on prompt) ---\n",
    "# In a full implementation, these might call separate LLMs or specific logic.\n",
    "@tool\n",
    "def retrieve_rubric_snippets(query: str, company_tag: str, top_k: int = 3, index_path: str = FAISS_RUBRIC_PATH) -> str:\n",
    "    \"\"\"Retrieves relevant rubric snippets from the FAISS index.\"\"\"\n",
    "    results = retrieve_from_vector_db(query=f\"{query} {company_tag}\", index_path=index_path, k=top_k)\n",
    "    return \"\\n\".join(results)\n",
    "\n",
    "# TODO Retrieve knowledge snippets from the FAISS index \n",
    "\n",
    "@tool\n",
    "def generate_ideal_answer(question: str, company_tag: Optional[str] = None) -> str:\n",
    "    \"\"\"Generates an ideal answer to the question (simulated by LLM call).\"\"\"\n",
    "    # This would typically involve another LLM call with specific instructions\n",
    "    # For simplicity here, we'll just return a placeholder or let the main agent handle it.\n",
    "    return f\"[Placeholder: Ideal answer generation for '{question}' considering company '{company_tag}']\"\n",
    "\n",
    "@tool\n",
    "def rewrite_candidate_answer(question: str, candidate_answer: str) -> str:\n",
    "    \"\"\"Rewrites the candidate's answer for improvement (simulated by LLM call).\"\"\"\n",
    "    return f\"[Placeholder: Rewritten version of answer for '{question}']\"\n",
    "\n",
    "@tool\n",
    "def critique_and_advise(question: str, candidate_answer: str, ideal_answer: str, company_tag: Optional[str] = None) -> str:\n",
    "    \"\"\"Provides critique and advice based on the answers (simulated by LLM call).\"\"\"\n",
    "    return f\"[Placeholder: Critique for answer to '{question}' considering company '{company_tag}']\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36b67c2",
   "metadata": {},
   "source": [
    "## 3. Agent Definitions\n",
    "\n",
    "Define the state, prompts, and nodes for each agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cd600388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Agent State ---\n",
    "class InterviewState(TypedDict):\n",
    "    messages: List[Any] # Stores the conversation history\n",
    "    user_resume_path: Optional[str]\n",
    "    user_jd_path: Optional[str]\n",
    "    user_resume_text: Optional[str]\n",
    "    user_jd_text: Optional[str]\n",
    "    clean_resume: Optional[str]\n",
    "    clean_jd: Optional[str]\n",
    "    company_name: Optional[str]\n",
    "    knowledge_output: Optional[Dict[str, Any]] # Output from Knowledge Agent\n",
    "    planner_output: Optional[Dict[str, Any]] # Output from Planner Agent (study plan, etc.)\n",
    "    preferred_question_type: Optional[str]\n",
    "    generated_question: Optional[str]\n",
    "    candidate_answer: Optional[str]\n",
    "    evaluation_output: Optional[Dict[str, Any]] # Output from Evaluation Agent\n",
    "    current_agent: str # Tracks which agent's turn it is\n",
    "\n",
    "# --- Agent Prompts (from design.md) ---\n",
    "\n",
    "preprocessing_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are PreprocessingAgent, a specialist in structuring resume and job-description data for downstream analysis.\n",
    "You will be given paths to the user's resume and the job description.\n",
    "\n",
    "1. Call `extract_text_with_ocr` for both the resume file (`user_resume_path`) and the job description file (`user_jd_path`).\n",
    "2. Clean the extracted text: Remove headers, footers, duplicate whitespace, and decorative lines. Store these as `clean_resume` and `clean_jd`.\n",
    "   *(Self-correction: Labeling sections like [CONTACT...] is complex and better handled by downstream agents if needed. Focus on cleaning and embedding.)*\n",
    "3. Extract the 'COMPANY NAME' from the cleaned job description (`clean_jd`).\n",
    "4. Call `generate_embeddings_and_save` **once for `clean_resume`**, saving to ../faiss/resume_embeddings.\n",
    "5. Call `generate_embeddings_and_save` **once for `clean_jd`**, saving to ../faiss/jd_embeddings .\n",
    "6. Return **only** the JSON object containing the cleaned JD and company name:\n",
    "   ```json\n",
    "   {{\n",
    "     \"clean_jd\": \"<cleaned job description text>\",\n",
    "     \"company_name\": \"<extracted company name>\"\n",
    "   }}\n",
    "   ```\n",
    "   *Do not include the raw embeddings or cleaned resume in the final JSON output of this step.*\n",
    "7. This output signals completion. The next agent (KnowledgeAgent) will be invoked by the graph.\n",
    "\"\"\"),\n",
    "    (\"human\", \"Process the resume at {user_resume_path} and the job description at {user_jd_path}.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "knowledge_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are the Knowledge Agent in a multi-agent interview preparation system. Your role is to extract real-world, subjective expectations about how top companies evaluate technical candidates during interviews. You must infer the evaluation rubric, communication expectations, common patterns, and failure modes using a web search tool.\n",
    "\n",
    "## Tools Available:\n",
    "\n",
    "### 1. `tavily_tool` (Powered by Tavily)\n",
    "- Performs web search queries across Reddit, Glassdoor, Blind, Medium, and other platforms.\n",
    "- Returns concise snippets and links, not full articles.\n",
    "- Snippets often contain behavioral signals or reflections from interviewees.\n",
    "\n",
    "### 2. `generate_embeddings`\n",
    "- Accepts a list of web snippets with metadata.\n",
    "- Converts them into embeddings.\n",
    "- Saves them to a FAISS index at `./faiss/search_results`.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs:\n",
    "- `target_company` (string): The company to analyze (e.g., \"Amazon\").\n",
    "- `clean_jd` (string): The job description text.\n",
    "- `max_snippets` (int): Max search results to retrieve (default: 20).\n",
    "\n",
    "---\n",
    "\n",
    "## Search Strategy:\n",
    "Construct at least 4 queries using variations like:\n",
    "- `{company} coding interview expectations site:reddit.com`\n",
    "- `{company} behavioral interview rubric site:glassdoor.com`\n",
    "- `{company} system design interview site:blind.com`\n",
    "- `{company} interview debrief site:medium.com`\n",
    "\n",
    "---\n",
    "\n",
    "## Goals:\n",
    "1. Use `websearcher_tool` to collect search result snippets about the company’s interview process.\n",
    "2. For each snippet, analyze and infer themes such as:\n",
    "   - Ownership\n",
    "   - Tradeoff Thinking\n",
    "   - Structured Reasoning\n",
    "   - Handling Ambiguity\n",
    "   - Communication Style\n",
    "3. Cluster the themes and summarize them as a list of `inferred_rubric` items.\n",
    "4. From the language and tone of the snippets, extract 3–5 actionable `communication_tips`.\n",
    "5. Record common topics or questions discussed in snippets (optional).\n",
    "6. Pass the snippet list and associated metadata into `generate_embeddings` to persist them in FAISS.\n",
    "\n",
    "---\n",
    "\n",
    "## Output Format:\n",
    "Return a JSON object in the format:\n",
    "```json\n",
    "{\n",
    "  \"company\": \"Amazon\",\n",
    "  \"inferred_rubric\": [\n",
    "    {\n",
    "      \"theme\": \"Ownership\",\n",
    "      \"evidence\": \"Multiple Reddit users reported being asked how they would proactively handle fallback strategies.\",\n",
    "      \"discussion_reference\": \"https://www.reddit.com/r/csMajors/comments/abc123\"\n",
    "    },\n",
    "    {\n",
    "      \"theme\": \"Tradeoff Thinking\",\n",
    "      \"evidence\": \"Glassdoor snippets emphasize discussing time-space tradeoffs during implementation decisions.\",\n",
    "      \"discussion_reference\": \"https://www.glassdoor.com/Interview/Amazon-Interview-Questions.htm\"\n",
    "    }\n",
    "  ],\n",
    "  \"communication_tips\": [\n",
    "    \"Narrate your approach clearly before coding.\",\n",
    "    \"Always explain tradeoffs when discussing solutions.\",\n",
    "    \"Discuss scalability and edge case assumptions upfront.\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "\"\"\"),\n",
    "    (\"human\", \"Generate interview insights for {company_name}. Resume context (optional): {user_resume_text}. JD context (optional): {clean_jd}\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are the Planner Agent, orchestrating interview preparation.\n",
    "**Inputs You Have Access To (implicitly via state or tools)**:\n",
    "- Parsed Resume Context (from ../faiss/resume_embeddings )\n",
    "- Parsed Job Description Context (from ../faiss/jd_embeddings )\n",
    "- Knowledge Agent Output: `inferred_rubric` and `communication_tips` for `{company_name}`.\n",
    "- User Preference: `preferred_question_type` (e.g., 'technical', 'behavioral').\n",
    "\n",
    "**Your Responsibilities**:\n",
    "1.  **Synthesize**: Briefly analyze the alignment between resume, JD, and company insights.\n",
    "2.  **Generate Study Plan**: Create a concise, actionable study plan (markdown format).\n",
    "3.  **Suggest LeetCode**: Call `company_leetcode_retriever` for `{company_name}`.\n",
    "4.  **Present Insights**: Format and include the `inferred_rubric` and `communication_tips` in your output.\n",
    "5.  **Embed Insights**: Call `generate_embeddings_and_save` to save the combined text of the rubric and tips to `{FAISS_RUBRIC_PATH}`.\n",
    "6.  **Prepare for Question Agent**: Note the `preferred_question_type` for the next step.\n",
    "\n",
    "**Output Format**:\n",
    "Return **only** a JSON object like this:\n",
    "```json\n",
    "{{\n",
    "  \"study_plan\": \"<Markdown formatted study plan>\",\n",
    "  \"suggested_leetcode\": [\"<LeetCode Q1>\", \"<LeetCode Q2>\"],\n",
    "  \"company_insights_display\": {{\n",
    "    \"company\": \"{company_name}\",\n",
    "    \"inferred_rubric\": [ ... ],\n",
    "    \"communication_tips\": [ ... ]\n",
    "  }},\n",
    "  \"embedding_status\": \"<Status message from generate_embeddings_and_save>\",\n",
    "  \"next_action\": \"Proceed to generate a '{preferred_question_type}' question.\"\n",
    "}}\n",
    "\"\"\"\n",
    "    ),\n",
    "    (\"human\", \"Plan the interview prep for {company_name} based on the available context and user preference for a '{preferred_question_type}' question. Knowledge Agent output: {knowledge_output}\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "question_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are the Question Agent. Your goal is to generate a single, relevant, open-ended interview question.\n",
    "**Inputs You Have Access To (implicitly via state or tools)**:\n",
    "- Question Type Requested: `{preferred_question_type}`\n",
    "- Company: `{company_name}`\n",
    "- Contextual Data (via `retrieve_from_vector_db` from ../faiss/resume_embeddings, ../faiss/jd_embeddings, `{FAISS_RUBRIC_PATH}`)\n",
    "\n",
    "**Your Task**:\n",
    "1. Call `retrieve_from_vector_db` using relevant queries (e.g., job title, key skills, company name, question type) against the resume, JD, and rubric indices to gather context.\n",
    "2. Synthesize the retrieved context.\n",
    "3. Generate **one** interview question of the type `{preferred_question_type}` that is:\n",
    "    - Tailored to the company (`{company_name}`).\n",
    "    - Relevant to the job description and candidate's likely experience.\n",
    "    - Aligned with the company's inferred rubric/culture (if available).\n",
    "    - Clear, professional, and open-ended.\n",
    "\n",
    "**Output Format**:\n",
    "Return **only** a JSON object like this:\n",
    "```json\n",
    "{{\n",
    "  \"question\": \"<The generated interview question>\"\n",
    "}}\n",
    "```\n",
    "\"\"\"),\n",
    "    (\"human\", \"Generate a '{preferred_question_type}' question for {company_name} based on the available context.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "evaluation_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are EvaluationFeedbackAgent, a senior interview coach.\n",
    "**Given**:\n",
    "- question: `{generated_question}`\n",
    "- candidate_answer: `{candidate_answer}`\n",
    "- company_tag: `{company_name}`\n",
    "- rubric_index_path: `{FAISS_RUBRIC_PATH}`\n",
    "\n",
    "**Do the following**:\n",
    "1. Call `retrieve_rubric_snippets` using the `question` and `company_tag` to get relevant evaluation criteria.\n",
    "2. Call `generate_ideal_answer` for the `question` and `company_tag`.\n",
    "3. Call `rewrite_candidate_answer` for the `question` and `candidate_answer`.\n",
    "4. Call `critique_and_advise` using all inputs.\n",
    "   - The critique must highlight strengths, list missed elements (e.g., complexity, STAR method), suggest improvements, use bullet points, and bold key terms.\n",
    "\n",
    "**Output Format**:\n",
    "Return **only** this JSON:\n",
    "```json\n",
    "{{\n",
    "    \"ideal_answer\": \"<Output from generate_ideal_answer>\",\n",
    "    \"improved_answer\": \"<Output from rewrite_candidate_answer>\",\n",
    "    \"detailed_feedback\": \"<Output from critique_and_advise>\"\n",
    "}}\n",
    "```\n",
    "\"\"\"),\n",
    "    (\"human\", \"Evaluate the answer '{candidate_answer}' for the question '{generated_question}' for company '{company_name}'.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "# Replace the LLM-based preprocess agent with the manual function\n",
    "async def manual_preprocess_agent_node(state: InterviewState, config: dict):\n",
    "    print(\"\\n🔧 Running preprocess agent manually...\")\n",
    "    resume_path = state[\"user_resume_path\"]\n",
    "    jd_path = state[\"user_jd_path\"]\n",
    "\n",
    "    resume_text = extract_text_with_ocr.invoke(resume_path)\n",
    "    jd_text = extract_text_with_ocr.invoke(jd_path)\n",
    "\n",
    "    clean_jd = re.sub(r\"[^\\S\\r\\n]+\", \" \", jd_text.strip())\n",
    "    clean_jd = re.sub(r\"(?m)^[=_\\\\-]{3,}$\", \"\", clean_jd)\n",
    "\n",
    "    company_match = re.search(r\"(?:at|join)\\\\s+([A-Z][a-zA-Z0-9& ]+)\", clean_jd)\n",
    "    company_name = company_match.group(1).strip() if company_match else \"Unknown\"\n",
    "\n",
    "    generate_embeddings_and_save.invoke({\"text\": resume_text, \"index_path\": FAISS_RESUME_PATH})\n",
    "    generate_embeddings_and_save.invoke({\"text\": clean_jd, \"index_path\": FAISS_JD_PATH})\n",
    "\n",
    "    state[\"user_resume_text\"] = resume_text\n",
    "    state[\"user_jd_text\"] = jd_text\n",
    "    state[\"clean_jd\"] = clean_jd\n",
    "    state[\"company_name\"] = company_name\n",
    "\n",
    "    print(f\"✅ Preprocess Complete. Extracted company: {company_name}\")\n",
    "    return {\"messages\": []}\n",
    "\n",
    "\n",
    "# --- Agent Nodes ---\n",
    "# Helper to create agent nodes\n",
    "async def preprocess_agent_node(state: InterviewState, config: dict):\n",
    "    print(\"\\n🔧 Running preprocess agent manually...\")\n",
    "\n",
    "    # Extract paths\n",
    "    resume_path = state[\"user_resume_path\"]\n",
    "    jd_path = state[\"user_jd_path\"]\n",
    "\n",
    "    # Extract raw text\n",
    "    resume_text = extract_text_with_ocr.invoke(resume_path)\n",
    "    jd_text = extract_text_with_ocr.invoke(jd_path)\n",
    "\n",
    "    # Simple clean of JD\n",
    "    clean_jd = re.sub(r\"[^\\S\\r\\n]+\", \" \", jd_text.strip())  # Remove excessive whitespace\n",
    "    clean_jd = re.sub(r\"(?m)^[=_\\\\-]{3,}$\", \"\", clean_jd)   # Remove decorative lines\n",
    "\n",
    "    # Try extracting company name (super basic fallback)\n",
    "    company_match = re.search(r\"(?:at|join)\\s+([A-Z][a-zA-Z0-9& ]+)\", clean_jd)\n",
    "    company_name = company_match.group(1).strip() if company_match else \"Unknown\"\n",
    "\n",
    "    # Generate embeddings and save\n",
    "    generate_embeddings_and_save.invoke({\"text\": resume_text, \"index_path\": FAISS_RESUME_PATH})\n",
    "    generate_embeddings_and_save.invoke({\"text\": clean_jd, \"index_path\": FAISS_JD_PATH})\n",
    "\n",
    "    # Update state\n",
    "    state[\"user_resume_text\"] = resume_text\n",
    "    state[\"user_jd_text\"] = jd_text\n",
    "    state[\"clean_jd\"] = clean_jd\n",
    "    state[\"company_name\"] = company_name\n",
    "\n",
    "    print(f\"\\n✅ Preprocess Complete. Extracted company: {company_name}\")\n",
    "    return {\"messages\": []}\n",
    "\n",
    "def create_agent_node(prompt: ChatPromptTemplate, tools: List[Any]):\n",
    "    agent = prompt | llm.bind_tools(tools)\n",
    "    tool_executor = ToolNode(tools)\n",
    "    async def agent_node(state: InterviewState, config: dict):\n",
    "        print(f\"\\n📤 Sending to {state.get('current_agent')}:\")\n",
    "        print(json.dumps(state, indent=2, default=str))\n",
    "\n",
    "        try:\n",
    "            result = await agent.ainvoke(state, config)\n",
    "            print(f\"\\n🧾 Raw LLM output for agent {state.get('current_agent')}:\")\n",
    "            print(result.content)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error during ainvoke for agent {state.get('current_agent', 'unknown')}: {e}\")\n",
    "            raise\n",
    "        # If tool calls are requested\n",
    "        if isinstance(result, ToolMessage) or (isinstance(result, AIMessage) and result.tool_calls):\n",
    "             # We delegate to the ToolNode\n",
    "            return {\"messages\": [result]}\n",
    "        else:\n",
    "            # If no tool calls, return the result directly\n",
    "            # Attempt to parse JSON output if expected\n",
    "            try:\n",
    "                if isinstance(result.content, str) and result.content.strip().startswith('{'):\n",
    "                    parsed_output = json.loads(result.content)\n",
    "                    # Update state based on agent\n",
    "                    agent_name = state.get('current_agent', 'unknown')\n",
    "                    if agent_name == 'preprocess':\n",
    "                        print(\"✅ Fields in parsed_output:\", list(parsed_output.keys()))\n",
    "                        state['clean_jd'] = parsed_output.get('clean_jd')\n",
    "                        state['user_resume_text'] = parsed_output.get('user_resume_text')  # Add this\n",
    "                        state['user_jd_text'] = parsed_output.get('clean_jd')\n",
    "                        state['company_name'] = parsed_output.get('company_name')\n",
    "                    elif agent_name == 'knowledge':\n",
    "                        state['knowledge_output'] = parsed_output\n",
    "                    elif agent_name == 'planner':\n",
    "                        state['planner_output'] = parsed_output\n",
    "                    elif agent_name == 'question':\n",
    "                        state['generated_question'] = parsed_output.get('question')\n",
    "                    elif agent_name == 'evaluate':\n",
    "                        state['evaluation_output'] = parsed_output\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: Could not parse JSON output from {state.get('current_agent', 'agent')}\")\n",
    "                # Fallback or handle error as needed\n",
    "            return {\"messages\": [result]}\n",
    "    return agent_node, tool_executor\n",
    "\n",
    "# Create nodes\n",
    "preprocess_tools = [extract_text_with_ocr, generate_embeddings_and_save]\n",
    "preprocess_agent_node, preprocess_tool_node = create_agent_node(preprocessing_prompt, preprocess_tools)\n",
    "\n",
    "knowledge_tools = [tavily_tool, generate_embeddings_and_save]\n",
    "knowledge_agent_node, knowledge_tool_node = create_agent_node(knowledge_prompt, knowledge_tools)\n",
    "\n",
    "planner_tools = [company_leetcode_retriever, generate_embeddings_and_save]\n",
    "planner_agent_node, planner_tool_node = create_agent_node(planner_prompt, planner_tools)\n",
    "\n",
    "question_tools = [retrieve_from_vector_db]\n",
    "question_agent_node, question_tool_node = create_agent_node(question_prompt, question_tools)\n",
    "\n",
    "evaluation_tools = [retrieve_rubric_snippets, generate_ideal_answer, rewrite_candidate_answer, critique_and_advise]\n",
    "evaluation_agent_node, evaluation_tool_node = create_agent_node(evaluation_prompt, evaluation_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a761bab",
   "metadata": {},
   "source": [
    "## 4. Graph Definition\n",
    "\n",
    "Define the workflow connecting the agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9cba1e9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Node `preprocess` already present.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m workflow.add_node(\u001b[33m\"\u001b[39m\u001b[33mpreprocess\u001b[39m\u001b[33m\"\u001b[39m, preprocess_agent_node)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Override the node with the manual version\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mworkflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_node\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpreprocess\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmanual_preprocess_agent_node\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# workflow.add_node(\"preprocess_tools\", preprocess_tool_node)\u001b[39;00m\n\u001b[32m      9\u001b[39m workflow.add_node(\u001b[33m\"\u001b[39m\u001b[33mknowledge\u001b[39m\u001b[33m\"\u001b[39m, knowledge_agent_node)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langgraph/graph/state.py:380\u001b[39m, in \u001b[36mStateGraph.add_node\u001b[39m\u001b[34m(self, node, action, metadata, input, retry, destinations)\u001b[39m\n\u001b[32m    378\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nodes:\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNode `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` already present.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m node == END \u001b[38;5;129;01mor\u001b[39;00m node == START:\n\u001b[32m    382\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNode `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` is reserved.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Node `preprocess` already present."
     ]
    }
   ],
   "source": [
    "# --- Graph Construction ---\n",
    "workflow = StateGraph(InterviewState)\n",
    "\n",
    "# # Add nodes for each agent and their tools\n",
    "workflow.add_node(\"preprocess\", preprocess_agent_node)\n",
    "# Override the node with the manual version\n",
    "workflow.add_node(\"preprocess\", manual_preprocess_agent_node)\n",
    "# workflow.add_node(\"preprocess_tools\", preprocess_tool_node)\n",
    "workflow.add_node(\"knowledge\", knowledge_agent_node)\n",
    "workflow.add_node(\"knowledge_tools\", knowledge_tool_node)\n",
    "workflow.add_node(\"planner\", planner_agent_node)\n",
    "workflow.add_node(\"planner_tools\", planner_tool_node)\n",
    "workflow.add_node(\"question\", question_agent_node)\n",
    "workflow.add_node(\"question_tools\", question_tool_node)\n",
    "workflow.add_node(\"record_answer\", record_and_transcribe_audio) # Direct tool call node\n",
    "workflow.add_node(\"evaluate\", evaluation_agent_node)\n",
    "workflow.add_node(\"evaluate_tools\", evaluation_tool_node)\n",
    "\n",
    "# Define edges\n",
    "workflow.set_entry_point(\"preprocess\")\n",
    "\n",
    "# Preprocessing Agent Logic\n",
    "# workflow.add_edge(\"preprocess\", \"preprocess_tools\")\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"preprocess_tools\",\n",
    "#     tools_condition,\n",
    "#     {\"continue\": \"knowledge\", END: END} # If tool call needed, loop back via tools_condition, else go to knowledge\n",
    "# )\n",
    "workflow.add_edge(\"preprocess\", \"knowledge\")\n",
    "\n",
    "# Knowledge Agent Logic\n",
    "workflow.add_edge(\"knowledge\", \"knowledge_tools\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"knowledge_tools\",\n",
    "    tools_condition,\n",
    "    {\"continue\": \"planner\", END: END}\n",
    ")\n",
    "\n",
    "# Planner Agent Logic\n",
    "workflow.add_edge(\"planner\", \"planner_tools\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"planner_tools\",\n",
    "    tools_condition,\n",
    "    {\"continue\": \"question\", END: END}\n",
    ")\n",
    "\n",
    "# Question Agent Logic\n",
    "workflow.add_edge(\"question\", \"question_tools\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"question_tools\",\n",
    "    tools_condition,\n",
    "    {\"continue\": \"record_answer\", END: END} # After question is generated, record answer\n",
    ")\n",
    "\n",
    "# Record Answer Node\n",
    "workflow.add_edge(\"record_answer\", \"evaluate\") # After recording, go to evaluation\n",
    "\n",
    "# Evaluation Agent Logic\n",
    "workflow.add_edge(\"evaluate\", \"evaluate_tools\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"evaluate_tools\",\n",
    "    tools_condition,\n",
    "    {\"continue\": END, END: END} # End after evaluation\n",
    ")\n",
    "\n",
    "# Compile the graph\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "\n",
    "print(\"Graph compiled successfully!\")\n",
    "# Optional: Visualize the graph\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph: {e}. Make sure graphviz and mermaid are installed/configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9011b1",
   "metadata": {},
   "source": [
    "## 5. Execution and Interaction\n",
    "\n",
    "Run the graph with user inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b111476e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2143958719.py, line 13)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mjd_file = \"amazon-jd.txt\"  ., \"/path/to/your/job_description.txt\"\u001b[39m\n                                ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import pprint\n",
    "\n",
    "# --- Execution ---\n",
    "\n",
    "# IMPORTANT: Set the path to your Google Cloud credentials file\n",
    "# This is needed for the record_and_transcribe_audio tool\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"key.json\" # Replace with the actual path to your key.json\n",
    "\n",
    "# --- User Inputs ---\n",
    "# !!! IMPORTANT: Replace these with the actual paths to your files !!!\n",
    "resume_file = os.path.join(os.getcwd(), \"placeholder_resume.pdf\") # e.g., \"/path/to/your/resume.pdf\"\n",
    "jd_file = os.path.join(os.getcwd(), \"amazon-jd.txt\") #\"/path/to/your/job_description.txt\"\n",
    "user_preferred_question_type = \"technical\" # Options: \"technical\", \"behavioral\", \"system design\", \"debugging/problem-solving\"\n",
    "\n",
    "# Create dummy files if they don't exist for the example run\n",
    "if not os.path.exists(resume_file):\n",
    "    with open(resume_file, \"w\") as f:\n",
    "        f.write(\"Sample Resume Content: Python Developer with 5 years experience in web development and data analysis.\")\n",
    "if not os.path.exists(jd_file):\n",
    "     with open(jd_file, \"w\") as f:\n",
    "        f.write(\"Sample Job Description: Looking for a Senior Software Engineer at Google. Requires strong Python skills, experience with distributed systems, and cloud platforms.\")\n",
    "\n",
    "# Define the initial state to start the graph\n",
    "initial_state = {\n",
    "    \"messages\": [],\n",
    "    \"user_resume_path\": resume_file,\n",
    "    \"user_jd_path\": jd_file,\n",
    "    \"preferred_question_type\": user_preferred_question_type,\n",
    "    \"current_agent\": \"preprocess\" # Start with the preprocessing agent\n",
    "}\n",
    "\n",
    "# Configuration for the graph run (e.g., unique thread ID)\n",
    "config = {\"configurable\": {\"thread_id\": \"interview-prep-thread-1\"}}\n",
    "\n",
    "async def run_graph():\n",
    "    final_state = None\n",
    "    print(\"--- Starting Interview Prep Workflow ---\")\n",
    "    print(f\"Resume: {resume_file}\")\n",
    "    print(f\"Job Description: {jd_file}\")\n",
    "    print(f\"Preferred Question Type: {user_preferred_question_type}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    async for event in graph.astream_events(initial_state, config, version=\"v1\"):\n",
    "        kind = event[\"event\"]\n",
    "        tags = event.get(\"tags\", [])\n",
    "        if kind == \"on_chat_model_stream\":\n",
    "            content = event[\"data\"][\"chunk\"].content\n",
    "            if content:\n",
    "                # Print LLM tokens as they arrive\n",
    "                print(content, end=\"|\")\n",
    "        elif kind == \"on_tool_start\":\n",
    "            print(f\"\\n--- Calling Tool: {event['name']} ---\")\n",
    "            print(f\"   Args: {event['data'].get('input')}\")\n",
    "        elif kind == \"on_tool_end\":\n",
    "            print(f\"--- Tool Result: {event['name']} ---\")\n",
    "            print(f\"   Output: {event['data'].get('output')}\")\n",
    "            print(\"-\" * 30)\n",
    "        elif kind == \"on_chain_end\":\n",
    "             # Check if it's the end of a specific agent node run\n",
    "            if event[\"name\"] in [\"preprocess\", \"knowledge\", \"planner\", \"question\", \"evaluate\"]:\n",
    "                 print(f\"\\n--- Finished Agent: {event['name']} ---\")\n",
    "                 # pprint.pprint(event['data'].get('output'), indent=2) # Print agent output if needed\n",
    "                 print(\"-\" * 30)\n",
    "\n",
    "\n",
    "        # Track the final state\n",
    "        if kind == \"on_graph_end\":\n",
    "            final_state = event['data']['output']\n",
    "\n",
    "\n",
    "    print(\"\\n--- Workflow Complete ---\")\n",
    "\n",
    "    if final_state:\n",
    "        print(\"\\n--- Final Results ---\")\n",
    "        # Extract and print key information from the final state\n",
    "        planner_output = final_state.get('planner_output', {})\n",
    "        evaluation_output = final_state.get('evaluation_output', {})\n",
    "\n",
    "        print(\"\\n**Study Plan:**\")\n",
    "        print(planner_output.get('study_plan', 'Not generated.'))\n",
    "\n",
    "        print(\"\\n**Suggested LeetCode:**\")\n",
    "        pprint.pprint(planner_output.get('suggested_leetcode', 'Not generated.'))\n",
    "\n",
    "        print(\"\\n**Company Insights:**\")\n",
    "        pprint.pprint(planner_output.get('company_insights_display', 'Not generated.'))\n",
    "\n",
    "        print(f\"\\n**Generated Question ({final_state.get('preferred_question_type', 'N/A')}):**\")\n",
    "        print(final_state.get('generated_question', 'Not generated.'))\n",
    "\n",
    "        print(\"\\n**Your Transcribed Answer:**\")\n",
    "        # The actual transcribed answer isn't directly stored in the state by the tool node,\n",
    "        # but it was passed to the evaluation agent. We print the placeholder for clarity.\n",
    "        # In a real UI, you'd capture the output of the 'record_answer' node.\n",
    "        print(final_state.get('candidate_answer', '[Answer was recorded and passed to evaluation]'))\n",
    "\n",
    "\n",
    "        print(\"\\n**Evaluation Feedback:**\")\n",
    "        print(\"\\n*Ideal Answer (Placeholder):*\")\n",
    "        print(evaluation_output.get('ideal_answer', 'Not generated.'))\n",
    "        print(\"\\n*Improved Answer (Placeholder):*\")\n",
    "        print(evaluation_output.get('improved_answer', 'Not generated.'))\n",
    "        print(\"\\n*Detailed Feedback (Placeholder):*\")\n",
    "        print(evaluation_output.get('detailed_feedback', 'Not generated.'))\n",
    "    else:\n",
    "        print(\"Workflow did not complete successfully or final state not captured.\")\n",
    "\n",
    "# Run the asynchronous function\n",
    "import traceback\n",
    "\n",
    "try:\n",
    "    await run_graph()\n",
    "except Exception as e:\n",
    "    print(\"\\nAn error occurred during graph execution:\")\n",
    "    traceback.print_exc()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
